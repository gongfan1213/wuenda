# Lab 1: Implementing self-editing memory from scratch
There are a variety of ways to enable long-term memory in LLM agents, such as RAG and recursive summarization. The MemGPT paper first introduced the notion of self-editing memory. Essentially, offload memory management to the LLM. After all, the LLM is the most "intelligent" part of our programs, so why not have the LLM figure out memory instead of hard coding some solution?

In this section, we'll walk through how to use OpenAI's tool calling to implement some simple memory management tools.

```
from helper import get_openai_api_key
openai_api_key = get_openai_api_key()
from openai import OpenAI
import os

client = OpenAI(
    api_key=openai_api_key
)
```
Section 1: Breaking down the LLM context window

LLM's have a context window (the set of tokens that go into the model) that is limited in size. That means we need to be smart about what we place into the context. Usually the context window for an agent is strucuted in a certain way. Depending on the agent framework, the structure will vary, but usually the context window contains:


A system prompt instructing the agent's behavior

A conservation history of previous conversations

Because context windows are limited, only some of the conversation history can be included. Some frameworks will also place a recursive summary in the context, or retrieve relevant messages from an external database and also place them into the context. In MemGPT, we also reserver additional sections of the context for:

A recursive summary of all previous conversations

A core memory section that is read-writeable by the agent

# A simple agent's context window

In the code below, you can see how we can define an agent with a system prompt. The system prompt will be included in every chat completions request as the first message, while later messages will change over time as the user and assistant exchange messages.

```
system_prompt = "You are a chatbot."
# Make the completion request with the tool usage
chat_completion = client.chat.completions.create(
    model=model,
    messages=[
        # system prompt: always included in the context window 
        {"role": "system", "content": system_prompt}, 
        # chat history (evolves over time)
        {"role": "user", "content": "What is my name?"}, 
    ]
)
chat_completion.choices[0].message.content
```

















