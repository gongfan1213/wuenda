# 3
```
0:02 Now it's time to start coding.
0:04 In this lesson, you'll build a chatbot and code its tools.
0:07 Before you start working with MCP, let's make sure you have a good foundation
0:11 with tools use and prompting large language models.
0:14 All right, let's code.
0:15 So let's get started building our chatbot.
0:17 And before we hop into building MCP servers, we're going to start
0:21 just by building a very simple application with a chatbot that makes
0:25 use of archive for searching for papers and finding some information.
0:29 If this is information that you're already familiar with, feel
0:32 free to skip this lesson and hop into building your first MCP server.
0:35 So let's get started by bringing in some libraries that we need.
0:38 So we're going to bring in the arxiv SDK.
0:40 This is going to allow us to start searching for papers.
0:43 We're then going to bring in the JSON
0:44 module for some formatting the OS module for environment variables.
0:48 We're going to bring in a little bit of typing so we can type our code.
0:51 And then we'll make sure we bring in the Anthropic SDK.
0:54 I'm going to first start
0:55 by defining a constant here called paper directory,
0:58 which is just going to be the string papers.
1:00 And this is what I'm going to be using for saving information to the file system.
1:04 I'm then going to bring in my first function here.
1:06 So let's go take a look at this.
1:07 This function is called search papers.
1:08 And it accepts some kind of topic and a number of results which defaults to five.
1:12 And what I'm doing here is searching for papers on archive.
1:15 And if you're not familiar with Archive is an open source repository
1:19 of many different published papers across a variety of domains,
1:22 from mathematics to science to many different disciplines.
1:26 So we're going to search for some papers,
1:27 and then we're going to return a list of paper IDs.
1:30 And we're going to then use those paper IDs
1:32 in another function to get more detail and summarization.
1:35 We're going to initialize our client.
1:36 And then we'll start searching for relevant articles.
1:39 We'll take the results of those search and we'll go ahead and create a directory
1:42 if it exists already, great.
1:43 If not, we'll go ahead and make it.
1:45 And we'll save this information to a file called papers info dot JSON.
1:49 What we're going to do is process each of these papers and create a dictionary.
1:52 And then we're going to go ahead and write that to our file.
1:55 This is going to give us back some IDs when we're done.
1:57 Let's go ahead and search for some papers for computers.
2:00 We'll see here. This is being saved to a file locally.
2:02 And we got a bunch of IDs that we can use to get some more information around.
2:06 We're going to go ahead and bring in our second function here
2:08 now to make use of this paper ID.
2:10 So we're going to define another function here called extract info.
2:13 Which is going to take in one of these paper IDs
2:16 it's going to look in our papers info JSON and give us back some information
2:19 about that paper.
2:21 And if it can't be found we'll go ahead and just return a string.
2:23 There's no saved information for that paper.
2:26 So I'm going to go ahead and grab this first ID right here.
2:29 Let's go ahead and run this function.
2:30 And then we'll go ahead and call this function
2:33 with that particular ID just to show you what this looks like.
2:35 We can see right here we're getting back some data
2:38 not only related to the title of this, but also the URL
2:41 for the PDF as well as a summary of this particular paper.
2:44 We're going to start with these two functions.
2:46 And then we're going to go ahead and start
2:47 bringing in these functions as tools for a large language model.
2:51 So what we're going to be able to do is pass in some tools for Anthropic's
2:55 Claude model.
2:56 We're then going to go ahead and build a small chatbot that takes in these tools
3:00 and knows when to call them and return the data, particularly for these functions.
3:04 So let's define our tools list.
3:06 If you are familiar with tool use, this should be nothing terribly new,
3:09 but every single tool that you make is always
3:11 going to have a name and a description
3:13 and then some kind of schema that it needs to follow.
3:15 So in this case, we have a tool for search papers and a tool for extract info.
3:19 Remember that the model itself is not going to call these functions.
3:23 We actually need to write the code
3:24 to call those functions and pass the data back to the model.
3:28 But these tools are going to allow the model to extend its functionality.
3:31 So instead of saying I don't know or hallucinate,
3:34 we're going to get back the answer that we want here.
3:36 So let's go ahead and start writing some of our logic
3:38 for working with our large language model and executing these tools.
3:42 Let's bring in a little bit of mapping for our tool.
3:45 And right here we've got a function that is going to map the tools
3:47 that we have to calling that underlying function.
3:51 What you can see here is we have a dictionary for each of our tool names.
3:54 These refer to the functions that we have below.
3:56 And then a handy helper function to then go ahead and call that particular function
4:00 and return the result to us in a variety of data types that come in.
4:03 Let's go ahead and start building in our chatbot right now.
4:06 That's going to start by bringing in any environment variables that we have
4:08 API keys, and then creating an instance of our Anthropic client.
4:13 We're going to need this
4:13 so that we can go ahead and make calls to our model and get back some data.
4:17 Let's go ahead and bring in our boilerplate function
4:20 to go ahead and start working with the model.
4:22 If you've worked with Anthropic
4:23 before or many other models, this is going to look very familiar.
4:27 We're going to start with a list of messages.
4:29 We're going to go ahead and pass in the query that the user puts in.
4:32 I'm going to talk to Claude 3.7 Sonnet right now.
4:34 We're going to go ahead and start a loop for the chat.
4:37 And if there is text data put that into the message.
4:40 If the data that's coming in is tool use, if the model detects
4:43 that a tool needs to be used,
4:44 we're going to go ahead and bring in our helper function for executing the tool
4:48 and then appending that tool result to the list of messages.
4:51 Let's go ahead and see this in action. Bring in our chat loop.
4:54 We've got all the functionality
4:55 we need to start working with tool use talking to our model.
4:58 Now let's start with a very simple example
5:00 for what it's going to look like to actually use this function.
5:03 We're going to run an infinite loop until we pass in the string quit.
5:06 So let's go ahead and start talking to our large language model.
5:09 Call our chat loop function.
5:11 Right now we can start putting in a query to start talking to our model.
5:14 So let's start with something very simple,
5:16 like just saying hi and make sure this works as expected.
5:19 We can see
5:19 the model here is going to let us know not only that it's an AI assistant,
5:22 but also let us know some of the tools that it has available.
5:25 This is excellent.
5:26 So let's go ahead and start making use of these tools.
5:29 Let's search for recent papers on a topic of interest.
5:32 So why don't we go and search for papers on algebra.
5:36 And this should make use of the tool that we have to go ahead and search
5:40 for papers.
5:41 We can see that topics being passed in and the results are saved here.
5:45 This is great.
5:46 It's even following up with would you like me to extract more detailed information.
5:49 So I'll go ahead and say yes please extract information
5:54 on the first two you found and summarize them both for me.
5:59 So what we're going to do is make use of that tool result that we got before.
6:02 And we're going to pass that in
6:04 and it's going to tell us which IDs it's interested in.
6:06 So I'm going to make sure
6:07 that I pass in those particular IDs so I can get that correctly done.
6:11 The IDs are here.
6:13 So we're going to see here
6:13 it's going to extract the info with these particular IDs.
6:16 And we're going to go ahead and get the result as well as a summarization here.
6:19 So we got some information about in variant algebras and deformation of algebras.
6:24 Honestly, I cannot tell you what this is.
6:26 But hopefully I can read the paper
6:28 and get up to speed on what this information has.
6:30 Something to remember is that there is no persistent memory here,
6:33 so as you go ahead and search for queries and get IDs,
6:36 nothing is going to be stored permanently.
6:38 So just make sure as you keep querying, you're passing in those IDs
6:42 and think of each conversation as brand new each time.
6:45 If you ever want to get out of this chat, remember we can always type in quit.
6:48 So make sure you run that and we'll see that we're all done here.
6:51 So what we've seen in this lesson is a review of large language
6:54 models, tool use and making use of the archive SDK.
6:58 What we're going to see shortly is how we can refactor this code
7:01 to turn those tools into MCP tools
7:04 to allow for a server to pass that information to us.
7:07 We're then going to test that server and see what the results look like.
7:10 I'll see you in the next lesson.
```
# 4
```
0:02 You'll now take the tools you implemented for the chatbot
0:04 and wrap them in an MCP server using the standard IO transport.
0:08 You'll use fast MCP, which provides a high level interface to build an MCP server.
0:13 Finally, you'll use the MCP inspector to test your server.
0:17 Let's get coding.
0:19 So we're going to pick up where we left off with the last lesson where we defined
0:22 two functions, search papers to go ahead and find papers on archive.
0:27 And then another function that we had down here,
0:30 extract info.
0:31 We took these functions and we defined them as tools
0:34 and passed them to our large language model.
0:36 What we're going to do now is abstract away the definition of these tools
0:41 and the schema of these tools, and create an MCP server
0:45 and use a library called fast MCP to help us build that quickly.
0:49 So with just a few lines of code, we're going to bring in fast MCP
0:53 We're going to define each of these functions as tools using MCP.
0:58 And then we're going to go ahead and start our MCP server
1:01 and test that in the browser.
1:02 So the first thing I'm going to do is bring in the necessary import that I need.
1:06 So from MCP dot server dot fast MCP
1:09 I'm going to import the fast MCP class.
1:12 I'm then going to go ahead and initialize that MCP server.
1:15 So I'll initialize our fast MCP server.
1:19 And I'll declare a variable here, which is the result of initializing that.
1:24 I'll give this server a name.
1:25 We'll call this research.
1:27 And let's go ahead and make use of that MCP variable.
1:31 As we saw before, there are quite a few primitives in the Model Context Protocol.
1:36 We saw tools, resources, prompts.
1:39 And what we're going to start with right now is just defining a tool.
1:42 And that's as easy as decorating this function with MCP dot tool.
1:47 We'll go ahead and make sure we do that for our function below as well.
1:51 And what this is going to do is define two tools
1:54 on our MCP server that we can start running and testing.
1:58 One last thing we need to do here
2:00 is make sure that we have the right command to start this server.
2:04 So what I'm going to bring in is a very standard bit of Python.
2:08 We're going to say if __name__ is equal to __main__,
2:11 this allows me to run this directly.
2:13 And if there are imports, this code does not run.
2:16 I'll go ahead and initialize and run the server.
2:19 I'll do that by calling MCP dot run.
2:22 And I'm going to pass in a transport.
2:24 As we saw before, we can run servers locally and we can run them remotely.
2:28 And when we're running our servers locally we almost always use standard IO.
2:33 So I'll pass in a transport of standard IO.
2:36 And that's all we need to start writing our MCP server.
2:39 You can see at the top here we've got a magic function to go ahead
2:43 and actually write a file called Research Server dot py.
2:47 So I'm going to go ahead and execute this cell.
2:49 And we're going to see that we write a file called Research server dot py.
2:53 This Python file is going to be used when we start our MCP server.
2:58 So let's go ahead and set up our environment and test our server.
3:01 To do this I'm going to go ahead and open up a new terminal.
3:04 In the environment that we're in,
3:06 we're going to need this code
3:07 necessary to create and run code inside of a terminal.
3:11 If you want to run this code locally on your own machine,
3:14 you're more than welcome to do so as well.
3:16 We can see here, I've got a terminal and I'm going to CD
3:19 into a folder called MCP project where my code lives.
3:22 I can see here I have my research server dot py.
3:25 In fact, this research server dot py is all of the code
3:29 that we just created above.
3:30 What I need to do here is install the necessary dependencies
3:35 to start working with MCP, as well as install the archive SDK.
3:39 What I'm going to do here is instead of using Pip,
3:42 I'm going to use a package manager called UV.
3:44 UV is slightly faster than Pip and provides quite a few other
3:47 nice tools to make it easier to manage your dependencies in Python.
3:52 Once I run uv init, we're going to see I have initialized
3:55 project called MCP project based on the name of the folder.
3:59 If you're familiar with virtual environments
4:02 or creating a virtual environment, this part is going to
4:04 look familiar if you're not familiar with virtual environments,
4:08 they're simply ways to self-contain the dependencies that you have.
4:12 So that you're not installing things globally and potentially conflicting
4:16 with other installations.
4:17 So let's go ahead and create our virtual environment using uv
4:21 venv We'll see here we have a virtual environment and actually
4:24 a folder called Dot venv
4:26 Let's go ahead and activate this virtual environment.
4:30 We'll do that using source dot
4:32 venv Then activate.
4:34 And I'm using tab completion here.
4:36 So I don't make any spelling mistakes.
4:38 We can see here now that we're in a virtual environment called MCP project.
4:43 And now we need to install the necessary dependencies.
4:46 I'll clear so you can see what we have at the top.
4:48 And I'll go ahead and bring in the dependencies of MCP and arxiv.
4:53 We'll give this a second to install.
4:54 And we're going to pull
4:55 in these dependencies
4:56 so that when we start running our MCP server we get the correct information.
5:00 The next step right now is to test our server file.
5:04 Instead of just running this code in Python, to test
5:07 the server, we're going to use a tool developed
5:10 called the Inspector, which gives us a browser-based environment
5:14 to explore the tools, resources, prompts, and other primitives that we have.
5:19 I'll clear here so we can start from the top. In order to use that tool,
5:22 I'm going to run the command npx at Model Context Protocol slash inspector.
5:27 What this is going to do is pull in the command to start this server
5:32 so that I don't have to install it locally.
5:34 And then the command that I want to use to run the application
5:38 is uv run research server. I'm using, uv run
5:42 so I can sure I have the correct dependencies
5:44 and I'm in my virtual environment.
5:46 This simply makes it an easier way to run Python files.
5:49 We'll see here
5:50 that we're starting our MCP inspector and the MCP inspector is up and running.
5:55 So I'm going to head over to the browser and hop into that particular inspector
5:59 that we're looking at.
6:01 When we take a look at this inspector,
6:03 we're going to see that we have a few different kinds of transport types.
6:07 We have server-sent events and Streamable HTTP, our remote protocols.
6:12 But remember our server is running on standard IO.
6:14 So let's keep that as is.
6:16 The command that we use to run is uv run research server.py.
6:21 We saw in the command line that's what we passed in
6:24 and that's being applied right here. One small note,
6:27 since we're running this in a slightly different environment, we're
6:30 going to have to paste in a proxy address that we've provided in the notebook.
6:35 If you're running this locally this is not something you'll have to do.
6:38 So I'm going to go ahead and paste in that proxy address.
6:41 And let's go ahead and connect to our server.
6:44 Once we connect, we're going to see some of the primitives that we have access to.
6:49 We discussed resources, prompts, and tools
6:52 and what we've created on this server right now are just some tools.
6:56 But what I do want you to see is this initialize process right here.
7:00 If you remember back in our previous lesson
7:03 when we spoke about communication and the transports,
7:06 the first process was a handshake or initialization from client to server.
7:11 We're going to head over to tools,
7:13 and we're going to go see what tools we have available.
7:15 This list tools is another command we can issue to go ahead
7:19 and find the particular tools that the server is providing.
7:23 We can also go ahead and run these tools.
7:27 So what's really nice about the inspector is without building
7:30 any kind of MCP client or host, you have a sandbox to play around with
7:34 the tools or prompts or resources that the server is returning.
7:38 We can also see here from the docstring, we've inferred a description
7:42 as well as the parameters that are required.
7:45 Let's go ahead and search for a topic.
7:47 I'll go search for chemistry and let's look for one result.
7:52 When I run that tool, we're going to go ahead and get back the return value.
7:57 So we have not yet
7:59 added any kind of large language model or functionality here.
8:03 We're simply just testing the MCP server.
8:07 I can go ahead and test my other tool with that paper ID,
8:11 and we should expect that I get back a success as well.
8:14 The MCP inspector is extremely valuable as you start building servers,
8:19 and even when you install servers that other people have written, it's a
8:22 great way to have a sandbox to play around once you're done using the inspector,
8:27 we can head back to our notebook and if you need to quit the server,
8:31 you can stop that process using Control+C.
8:34 And we're back in the terminal.
8:36 If you ever need to start that again, you can always press up
8:40 to get access to your previous command, so you don't have to take the whole thing.
8:43 In the next lesson, we're going to start layering on an MCP host and a client
8:48 and integrate this MCP server with a chatbot that we build all talking with MCP.
8:54 See you there.
```
# 5
```
0:01 With your MCP server ready,
0:03 it's now time to create an MCP client inside your chatbot.
0:07 To let the chatbot communicate with the server and get access to the tool
0:11 definitions and results.
0:13 Let's have some fun!
0:14 Now that we've seen how to build a server with MCP,
0:18 let's go ahead and move past the inspector and build our own host
0:22 to contain a client to talk to our MCP server.
0:26 We're going to be working with the chatbot directly, but if you want to take
0:30 a look at other files like the server that we've made before, feel free to do so.
0:35 We're going to start by revisiting what we saw before in our chatbot example.
0:39 You're going to
0:39 see a lot of this code again, but we're going to layer on a little bit more
0:43 as we start bringing a client into the mix.
0:45 Everything you're seeing here we've seen before.
0:48 This is simply the ability to process a query
0:51 using Claude 3.7 Sonnet, as well as tool use.
0:55 We can see here that we're not actually defining any tools.
0:58 That's all being done in the server that we made in the last lesson.
1:03 Let's go ahead now and start talking about how to bring in
1:06 and create an MCP client.
1:08 I'm going to bring in a bit of code from the underlying MCP library,
1:13 because I want to talk through what's actually happening here.
1:16 If you remember, when we create an MCP client, which lives inside of a host,
1:22 we need to make sure that that client establishes a connection to an MCP server.
1:28 An important note here,
1:29 the code that we're looking at is slightly lower level.
1:32 You won't always find yourself
1:34 building clients from scratch, but it's really important
1:37 that when you see other tools like Claude desktop or Claude AI,
1:41 you have an idea of what's happening under the hood.
1:44 So if this code looks relatively intimidating, don't be too worried.
1:48 We'll go step by step.
1:49 The goal here is really to make sure you understand how clients are created
1:54 and how they establish connections to servers.
1:56 What we're seeing here are a few imports from the underlying MCP library,
2:02 to bring in the necessary classes, to establish a connection to a server,
2:07 as well as the ability to start a subprocess from the client.
2:11 So the first thing we're going to do here is establish the server
2:15 and the parameters necessary that we want to connect to.
2:18 And this is actually going to look pretty familiar. That scene command that we ran,
2:22 uv run Research Server dot py.
2:25 We're specifying here to let our client know how to start the server.
2:30 If there are any environment variables that we need, we can pass those in here.
2:35 The next step is to actually establish that connection
2:38 and launch the server as a subprocess.
2:41 Since we might not want this to be blocking, we're going to be making
2:44 use of async and await quite a bit in Python.
2:48 If you're not too familiar with that,
2:49 no worries, I'll walk you through what needs to be done here.
2:52 We're going to define a function called run, and we're going to set up
2:55 a context manager to first pass in the parameters
3:00 from our server and establish a connection as a subprocess.
3:04 Once we've established the server to connect to,
3:07 we're going to get access to a read and write stream
3:11 that we can then pass to a higher level class called the client session.
3:16 In this client session, when we pass the read and write stream,
3:20 we'll get access to an underlying connection
3:23 that allows us to make use of functionality
3:26 for listing tools, initializing connections,
3:29 and doing quite a bit more with other primitives.
3:32 The first thing we're going to do
3:33 is establish that handshake and initialize our session.
3:37 Well, then go ahead and list all of the available tools
3:41 that the server is providing.
3:43 Remember, the client's job is to query for tools
3:46 and take those tools and pass them to a large language model.
3:50 We'll make use of our chat loop functionality that we saw before.
3:53 And if there is a tool that needs to be invoked,
3:56 we'll go ahead and let the MCP server do that work.
4:00 So we're going to see a slightly different bit of code
4:03 for executing the underlying tool.
4:06 We're going to bring in the tools from the MCP server.
4:09 And if a tool needs to be executed,
4:12 we'll let the MCP server know what to do.
4:15 And we've defined all the code necessary
4:17 in the previous lesson for what happens when that tool is executed.
4:22 Since we're working in an async environment,
4:24 we're going to be moving past MCP dot run and using async IO dot run.
4:30 So with that in mind, let's put this all together.
4:32 We're going to go ahead and add our MCP client to our chatbot.
4:37 We're going to go ahead and write a file called MCP chatbot dot py.
4:42 Since this is what we're going to run in the terminal
4:45 to start interacting with our chatbot, we're going to bring in all of the imports
4:49 that you saw before alongside nest async IO, which is necessary for different
4:55 operating systems to work properly with the event loop in Python.
4:59 We're going to bring in any environment variables that we have
5:02 and then initialize our chatbot.
5:03 When we initialize our chatbot, we don't have a current session
5:08 and we don't have any tools available to us.
5:11 We're going to see that once we start establishing
5:13 the connection, these values will change.
5:16 Our process query looks very similar to above
5:19 with a slight difference of what happens when the tool needs to be invoked.
5:24 We're using the session established to go back
5:27 to the MCP server and execute the tool necessary.
5:31 We're then going to follow similar logic for appending a message
5:34 and making use of tool use that we've seen before.
5:37 Our chat loop as well is going to look very similar.
5:40 We're going to go ahead and keep running until someone types in quit
5:44 and process that particular query whenever data comes in.
5:48 To wrap this up, we're going to define a function called connect
5:51 to Server and Run, which does just that. Like we saw before,
5:56 we establish a connection to an MCP server.
5:59 We get access to the read and write stream and the underlying session
6:04 so that we can establish that connection, list the tools that we need,
6:08 and then take those tools and pass them for tool use in the model.
6:13 To wrap this up, we initialize our chatbot
6:16 and we call our connect to server and run function.
6:19 Inside of a __name__ equals __mean__
6:22 We run our main function using async IO.
6:25 So let's go ahead and run this code to create the necessary MCP chatbot.py file.
6:31 So let's go ahead and bring in our terminal.
6:34 And we'll see here I'm in the L5 directory.
6:37 I'm going to CD into the MCP project folder.
6:42 And if we take a look at what I have right here,
6:45 I have a virtual environment that already exists.
6:49 So I'm going to go ahead and start by activating
6:52 that virtual environment. Source dot venv bin activate.
6:56 We're also going to need a couple other dependencies to make this project work.
7:00 So I'll go ahead clear this and I'll add the Anthropic SDK,
7:06 the python-dotenv module
7:08 for environment variable access and nest async IO.
7:12 Once I add those dependencies, I should have everything necessary
7:16 to start my chatbot. Before I start the chatbot
7:19 let's just make sure we see how this is coming together.
7:21 When I type in uv run MCP chatbot dot py.
7:25 We are going to connect to our MCP server,
7:29 make use of the tools that are defined, past those tools to Claude,
7:34 and then create a nice interface for us to start talking with Claude,
7:38 to get access to those tools and any other data that we want.
7:42 We can see here that when there is a connection,
7:45 we're processing that request of list tools request.
7:48 This is the underlying functionality in the protocol
7:51 that allows me to pull in the tools necessary.
7:54 We've connected to the server
7:56 with the following tools and we can start talking to our chatbot.
7:59 We can always start with something simple.
8:01 Just make sure things are working.
8:03 A friendly query to greet our chatbot.
8:05 Now let's go ahead and make use of some of those tools that we have.
8:09 So I'll ask, can you search for papers around physics
8:15 and find just two of them for me?
8:18 What we're going to do here is make use of those particular tools that we have.
8:23 We're going to see here using the call tool request
8:26 that the MCP client is sending this data to the server.
8:30 The server is invoking that tool and returning it back to us.
8:34 We're then using Claude with that additional context
8:37 to return a nice summary to us.
8:40 While we've done a little bit of lower level programing to make this work,
8:44 we've started to build a foundation for something incredibly powerful.
8:48 We're going to first establish multiple client sessions
8:51 to allow for the use of many different MCP servers.
8:54 So these can all start to work together.
8:56 And then we're going to start layering on additional primitives
8:59 like resources and prompts.
9:01 To really see this work at a much larger scale.
9:04 See you in the next lesson.
9:06 And don't forget, if you ever want to get out of the chatbot, you can
9:09 always type quit.
```
# 6
```
0:01 In the previous lesson, you connected your chatbot to one server that
0:05 you built.
0:06 Now you'll update your chatbot so that it can connect to any server.
0:10 You'll learn more about the reference servers developed by the Anthropic team
0:14 and how you can download them.
0:16 Let's get to it!
0:17 So far, we've seen how to build MCP servers
0:20 as well as clients and connect those on a 1 to 1 basis.
0:24 Well, we want to start introducing now, is the ability to not only build
0:29 multiple clients that can work with multiple servers, but also introduce
0:33 the entire ecosystem of servers that exist out there.
0:37 So I'm going to start here by taking a look at some of the servers
0:40 that are reference servers from Anthropic on our repository.
0:44 So let's go take a look on GitHub for the reference servers that we have
0:48 with the Model Context Protocol.
0:50 As you take a look through all of the servers here,
0:53 there's a massive, massive list.
0:55 So we're just going to start with the reference servers.
0:57 These are ones that we have worked on and built at Anthropic.
1:01 There are also many different third party servers and official integrations.
1:05 Any data source that you can imagine talking to at this point
1:08 probably has an MCP server.
1:11 Instead of you having to download these servers and run them locally.
1:14 We're also going to see how we can add the command necessary to run the server
1:19 without that much hassle.
1:20 The servers that we're going to be using
1:22 are the fetch server, as well as the file system server.
1:25 So let's take a look at the fetch server.
1:27 What's so interesting about this is that if you look
1:30 at the underlying source code for many of these servers, it's
1:34 actually going to look pretty familiar to what you built before.
1:37 We can see here that the fetch MCP server exposes tools
1:41 and a prompt to us, and we can see what the installation is as well.
1:45 Since this server is written in Python, we're actually going to use the uv command
1:51 to directly run a command called MCP server fetch, which will run all the code
1:56 necessary to download what we need and establish the connection.
2:00 So instead of uv run, we're going to be using uvx.
2:03 The fetch server allows us to retrieve content from web pages,
2:06 convert HTML to markdown so that LLMs can better consume that content.
2:12 The second server that we're going to be looking at is the file system server.
2:15 And just like you can imagine,
2:17 this is going to be a way for us to access our file system reading,
2:20 writing files, searching for files, getting metadata and so on.
2:25 We can see here there are resources and tools exposed,
2:28 quite a few different tools for reading and writing files.
2:32 If you take a look at the source code here,
2:34 you can see that this is not written in Python.
2:36 This is in fact written in TypeScript, which means instead of uvx,
2:40 we're going to be using a slightly different command.
2:42 If we take a look at the installation instructions to run this,
2:46 the command necessary is npx /y.
2:50 So that we don't need to press enter for any other installation instructions
2:54 and then Model Context Protocol server file system.
2:57 So similar
2:58 to running uvx where we can download what we need and execute it right away,
3:02 we're going to be using npx from the npm package manager.
3:06 We then specify any paths that we allow for reading and writing files into.
3:11 As you can see,
3:12 each of these reference servers have a bit of configuration required.
3:16 The name of the server, the command necessary, and so on.
3:19 So what we're going to do is we're going to make some updates to our chatbot.
3:23 Instead of hardcoding these server parameters.
3:26 We're going to make a small JSON file that we can read from
3:30 to figure out the necessary commands to interact with our servers.
3:34 We'll be using the file system server,
3:37 the research server that we're building, as well as the fetch server.
3:40 And we'll see how we can put all three of those together
3:43 to create very powerful prompts. In order to make this happen,
3:47 we're going to have to go ahead and change the code in our MCP chatbot.
3:51 The reference servers stay the same.
3:54 Our research server stays the same,
3:56 but we have to update the code a bit for our MCP chatbot.
3:59 There is a good amount here
4:00 that is relatively lower level and plenty of opportunity for refactoring.
4:05 So I'll walk you through what we have here
4:07 and I welcome any changes you'd like to make to grow this as it scales.
4:11 In order to get this to work, we're going to have to set up our own JSON
4:15 file to configure how we want to connect to each of the individual servers.
4:19 And here's what that's going to look like.
4:21 We're going to start with a little bit of JSON to contain all of our servers.
4:25 Then we'll specify the name of those servers
4:28 as well as the underlying command necessary.
4:31 And any arguments required for the research server.
4:35 This is going to look relatively familiar.
4:37 And for the reference servers,
4:38 since we're not downloading it and then running it locally ourselves,
4:42 we're using commands like npx and uvx to run those immediately.
4:46 So we're going to see this file.
4:48 And you can find this file as well in your list of files for this lesson
4:52 under the folder MCP project. For the File System server,
4:56 if you remember, we had to specify the paths that we wanted access to.
5:01 And here we're specifying a dot which means the current directory that we're in.
5:05 So this is not going to be able to read
5:07 or write from files or folders outside of this current directory.
5:11 Now let's go ahead and take a look at the code necessary for our MCP chatbot
5:16 to not only connect to multiple servers with multiple clients,
5:21 but also correctly read the JSON file for the server configuration necessary.
5:26 Let's go ahead and see what
5:27 we need to update our MCP chatbot to handle these connections.
5:31 If you take a look at the code that we have here for our MCP chatbot,
5:35 there's quite a bit more happening under the hood, and especially some lower
5:39 level ideas that I want you to not feel too intimidated by.
5:42 The most important takeaway here is to understand how tools like Claude
5:47 Desktop, Claude AI, Cursor, Windsurf, work under the hood
5:51 when they set up multiple connections to multiple servers.
5:55 What I'm going to do here is start by adding a little bit more to my MCP
5:58 chatbot.
5:59 I'm going to maintain a list
6:00 of all of the sessions that I've connected to, as well as all of the tools
6:04 and the particular session that that tool is related to.
6:08 Again, this is not production ready.
6:10 This is really just giving you a sense of how to get started.
6:13 And the focus here is to make sure that we correctly map a tool
6:16 to the session that we're working in.
6:18 We have a type definition
6:19 here, as our tools are a little bit more complex than we had before.
6:22 We're going to have some similar code to connect to a server, except that
6:26 since we have multiple context managers inside of an asynchronous environment,
6:31 we have to set up our connection a little bit differently.
6:34 So we use an async exit stack to manage our connections for reading and writing,
6:39 as well as managing the entire connection to the session.
6:43 Below, we're going to see some pretty familiar code.
6:46 We initialize a session, we list those tools,
6:49 and we take the tools and append them to our list of available tools.
6:53 You can imagine that this is a function that's going to be run
6:56 multiple times for each of the servers that we want to connect to.
7:00 And that is exactly what we're doing down here.
7:02 We're going to go ahead and read from our server config file.
7:05 We're going to parse that JSON,
7:07 turn it into a dictionary that we can then iterate over.
7:10 And for each individual MCP server, connect to it.
7:14 If you are familiar with asynchronous programing
7:17 you can see that this code is blocking,
7:19 and maybe you could refactor this to use async IO gather or so on.
7:23 But again, the focus here is understanding conceptually what's going on
7:27 and welcome any refactor as you'd like to do.
7:29 Once we connect to all of these servers, we're then going to use some logic
7:33 that looks pretty familiar as well.
7:35 We're going to go ahead and get access to our model.
7:38 We're going to pass in any information coming in from a query.
7:41 And then if there is a tool
7:43 that we need, we're going to go find it and call that particular tool.
7:47 The rest of this logic is very familiar.
7:49 The chat loop that we have is exactly as what we had before, with one small note,
7:54 that when we need to go ahead and close any connection that we have.
7:58 We do this using our context manager for multiple different connections.
8:03 Our main function has a little bit more to allow us to connect
8:06 to all of the servers that we need, and then start the chat loop.
8:10 And once that's all done,
8:11 we can go ahead and clean up any lingering connections that we have
8:15 to these servers.
8:16 And just like we had before, we're going to start this application
8:19 by calling Async io dot run with our main function.
8:22 So let's go ahead and write this file
8:24 and we'll hop back to the terminal in the terminal here.
8:27 I'm going to first CD into MCP project.
8:30 And I'm going to see here that I have again a dot then folder.
8:33 So let's go ahead and activate that virtual environment.
8:36 Source dot venv bin activate.
8:39 And then let's go ahead and run our chatbot.
8:42 I'll clear so we can take this from the top.
8:44 And I'll type in.
8:45 You've run MCP
8:48 chatbot dot py.
8:52 What we're going to do here
8:53 is connect to multiple MCP servers by setting up multiple clients.
8:58 We can see here, we've connected to the file system
9:01 with the allowed directory of the current directory we're in.
9:04 We've connected with these particular set of tools.
9:07 We've connected to our research server as well as the fetch server itself.
9:12 We have the same exact chat interface that we had before.
9:15 So I'm going to paste in this prompt
9:16 where I'm going to ask it to fetch the content of the Model Context Protocol
9:20 and save the content to a file called MCP summary,
9:23 and then create a visual diagram that summarizes the content.
9:27 So what we're going to be doing here
9:28 is use a multitude of tools to fetch information
9:31 and then to summarize that information.
9:34 We're then going to have it draw a nice little diagram for us.
9:36 So let's go take a look at what that looks like.
9:39 We can see here it's saved to a file called MCP summary MD.
9:43 So in our file system let's go take a look at what that file looks like.
9:47 So we've got this nice little diagram here for the Model Context Protocol.
9:51 This was done by fetching information from the website summarizing
9:55 that information.
9:56 Turning it into a nice visualization.
9:58 And again we're going to see an even prettier one
10:01 when we start bringing in tools like Claude Desktop.
10:03 But now the UI is totally up to you for what you want to do.
10:06 But you can do whatever you want with this file right now.
10:10 So we've seen how a couple of these servers can work together.
10:13 Let's try bringing all three together.
10:15 So we'll say fetch DeepLearning.AI
10:18 find an interesting term
10:22 to search papers around
10:26 and then summarize your findings
10:29 and write them to a file
10:32 called results dot txt.
10:35 We're going to make use of the fetch tool here to visit a website
10:40 and find the content of that website.
10:43 Based on that content, we'll find some interesting terms.
10:46 In this case we've got multi-concept pre-training.
10:50 We're then going to go ahead and find papers related to
10:52 that.
11:01 We're going to take that data and we're going to write it to a file.
11:05 You might not find yourself using a combination of these servers
11:08 for many real-world use cases, but now your imagination can carry you.
11:13 Any existing MCP server
11:15 can be added with minimal configuration,
11:18 and you can take the results of these different MCP servers to add
11:22 all the context you need to connect models like Claude to the outside world.
11:28 We can see here we've got a really nice summary.
11:31 Let's go ahead and see what's been written here.
11:33 So we got a very interesting result from our research.
11:35 It seems that while MCP or Model Context Protocol is a very powerful tool,
11:40 there also is another acronym for MCP for Multi Concept pre-training.
11:44 So it looks like the model got a little bit confused here.
11:47 When in doubt, this is why prompt engineering is so important.
11:49 And we could even follow up with a follow of
11:52 this is why you should include the Model Context Protocol and not other concepts
11:56 as well.
11:56 As always, if we want to leave this chat session, we can type in quit.
12:00 Now that we have multiple servers connecting to multiple clients,
12:04 let's start adding on a few other primitives like resources
12:07 for read only data
12:09 and prompt templates for the ability to generate prompts on the server
12:13 that the user can use
12:15 so that they don't have to write prompts completely from scratch.
12:18 I'll see you in the next lesson.
```
# 7
```
0:02 So far,
0:02 your MCP server only provided tools to your chatbot.
0:06 You'll now update your server so that it also provides
0:09 resources and a prompt template. On the chatbot side,
0:13 you'll expose those features to the user.
0:15 Let's do it!
0:16 We've already seen how to create multiple
0:19 MCP clients connecting to multiple MCP servers.
0:23 Now let's shift back to some of the other primitives in the protocol,
0:26 like resources and prompts, and talk about how we can add that both on the server
0:32 as well as on the ability of the client to consume that data.
0:36 In our research server dot py.
0:38 You can find all of these files in your file system.
0:41 All of these files are provided to you.
0:43 So what I'd love to do is walk through some of the code,
0:46 both on the server side and with our clients.
0:49 As we saw before, adding a tool is as easy as decorating MCP dot tool.
0:54 Now let's bring in resources and prompts and we'll talk a bit how to add those.
0:58 The code here right now is living on our server.
1:01 And what we're going to do is bring in a couple resources
1:05 for all of our particular folders,
1:08 as well as any papers on a particular topic.
1:11 Remember that resources are read-only data
1:14 that the application can choose to use, or we can give to the model.
1:18 So instead of making tools to go and fetch things from the file system
1:21 the same way, we have a Get request to fetch our data with HTTP,
1:25 we're going to do the same thing with resources.
1:27 So on the server, I have a resource with a URI for papers colon slash slash
1:32 folders to go ahead and list the available folders in the paper directory.
1:36 I also have a resource here to fetch information about a particular topic.
1:41 We haven't done any of the implementation yet for what this is going to look like,
1:44 how it's going to be presented, or how it's going to be fetched.
1:47 All we're setting up on the server
1:49 are just ways to listen for requests for these particular resources.
1:53 We have a little bit of string manipulation in here, as well
1:56 as reading files to go ahead and fetch the data necessary
2:00 with some error handling to make sure that if papers are not found,
2:04 we go ahead and put that error message in.
2:06 We can see here we're reading from our paper's info JSON file,
2:10 and then returning a bit of text around the content that we have.
2:13 Aside from our resources,
2:15 we can also add prompt or prompt templates to our MCP servers.
2:19 So let's take a look at the prompt that we have here.
2:21 Before we dive into this code, let's remember the purpose of the primitive
2:25 for a prompt template.
2:26 Prompts are meant to be user-controlled.
2:28 You can imagine as the user of an AI application,
2:31 you don't want to have to do complex prompt engineering yourself.
2:34 In fact, you may be working with a server.
2:37 You may be trying to get some information, but you might not know the best way
2:40 to fetch it or retrieve it based on the prompt that you have.
2:43 Prompt templates are created on the server and are sent to the client
2:48 so that the user can use those entire templates
2:51 without having to do all the prompt engineering on their own.
2:54 So instead of asking the user to just specify how to search for papers,
2:58 we're actually going to provide to them a battle-tested prompt
3:02 that includes the dynamic information that they can put in, like the topic
3:06 or the number of papers you can imagine. We can do
3:08 some pretty sophisticated evaluations and prompt engineering testing.
3:12 And by the time it gets to the user, this is abstracted away.
3:15 We create a prompt template by decorating a function with MCP dot prompt.
3:20 And then we return what the prompt template looks like.
3:23 All that we're going to do on the client side is have the user put in the number of
3:28 papers, which is optional, and the topic which is going to be required.
3:32 Now that we've seen what's going to be sent
3:34 from the server to the client, let's make sure we figure out now
3:37 how to start bringing in these resources and prompts, and how to create a
3:41 UI for what the resources and prompt templates should look like.
3:45 This UI that we create, this presentation that we create
3:49 is completely up to you as the developer to make.
3:52 What's so powerful about MCP is that it doesn't mandate
3:56 that all interfaces look the same and work in same.
3:59 We're simply focused on sending back data and manipulating data,
4:03 and the presentation is up to the client and the host to create.
4:07 So with that in mind, let's hop back to our chatbot.
4:10 As we saw before,
4:11 there is going to be some slightly lower level code happening here.
4:15 Fortunately, it's going to be relatively similar to what we saw before.
4:19 We're going to store a list of the available tools and prompts
4:22 that we have, as well as all of the URIs that we have for our particular resources.
4:28 We're going to see in our connect to server function
4:30 that things look pretty similar to what we saw before.
4:34 We're going to be using this exit stack to manage
4:36 all of our connections in an asynchronous environment.
4:39 We're going to initialize the session.
4:41 And then instead of just getting access to the tools,
4:44 we're going to do the same thing for our prompts and our resources.
4:47 We're going to go ahead and use the session that we establish
4:50 for each client to list the prompts, list the tools and list the resources.
4:55 If that server does not provide prompts or resources,
4:58 we'll handle that error and print that exception.
5:01 If there are any issues connecting to the server, we'll handle that as well.
5:05 Our connect to servers function looks similar to what we saw before.
5:09 We're going to read our JSON file, load in all of the names
5:12 of the servers and the configuration necessary.
5:16 Our process query is also going to look relatively similar.
5:19 We're going to go ahead and create a message with our available tools.
5:23 If we're using tool use we'll append that information.
5:26 And then we'll go ahead and make sure that we call the correct tool.
5:30 Where things will look slightly different
5:31 is where we start handling resources and prompt templates.
5:35 So let's start with resources. To get an individual resource,
5:39 we're going to go ahead and make sure that we're dealing with the correct URI.
5:43 And once we have that correct URI, we're going to read the resource from that URI.
5:48 All that we're doing here
5:49 is simply printing out the content of that particular resource.
5:53 But depending on your interface that you want to build,
5:56 you could do whatever you want with that data.
5:58 We're going to do a similar thing for listing our prompts.
6:00 We're going to go ahead and find all of the available prompts that we have.
6:04 And if there are any arguments that those prompts require,
6:07 we're going to go ahead and show that to the user. When a prompt comes in,
6:11 we're going to go ahead and execute it.
6:13 We'll see shortly what it looks like for a resource and a prompt to come in
6:17 for the particular session that we're in.
6:18 We fetch that prompt.
6:20 We go ahead and we execute that particular prompt with that query.
6:24 The function here that we have for executing
6:26 the prompt is going to require us to get access to the prompt name
6:30 and any arguments that it might have.
6:32 Once we fetch that particular prompt,
6:34 we go ahead and pass it in as the content of our message,
6:37 and we go ahead and process the query with those arguments.
6:41 Where things look a little bit different is our chat loop.
6:44 Here is where we're going to start adding in the particular user interface
6:48 for getting access to our resources and our prompts.
6:51 We're doing a little bit of string manipulation here,
6:53 and this is totally up to you as the developer
6:56 of the host and clients for how you want things to be presented.
7:00 We're going to be using the @ sign to get access to a particular resource.
7:04 And if we see that
7:05 there is a topic that's passed in first, we'll fetch it using that URI.
7:09 If we see that our query starts with a slash, this is how will denote
7:12 that we're using a particular prompt.
7:14 If the command is slash prompts, we'll show the user all of them.
7:18 If the command is slash prompt, we'll go ahead and make sure
7:21 we're passing in those arguments. To pass in those arguments,
7:24 we're doing a little bit of string manipulation as well.
7:27 We're looking for key-value pairs separated by an equal sign.
7:31 And once we have what we need we execute the prompt.
7:34 We have similar cleanup logic to what we saw before
7:37 and similar logic to connect to our chatbot.
7:40 That's a lot of code.
7:41 So let's take a step back and then we'll see this in the terminal.
7:43 So let's go ahead and get my terminal.
7:46 And we'll
7:46 see here, that I'm inside of the L7 folder.
7:50 Just like we saw before,
7:51 I'm going to CD into MCP project.
7:54 I'll make sure I have my dot env folder which it looks like I do.
7:58 So let's go ahead and activate the virtual environment. Source,
8:02 venv bin activate.
8:04 Now that we got this activated let's go ahead and run our chatbot.
8:08 uv run
8:11 MCP chatbot.py
8:15 What we're going to see here is that we're going to connect
8:18 to many different MCP servers.
8:20 We have a little bit of error handling here in case these servers
8:23 do not provide tools, resources or prompts.
8:27 What we see here is not only the ability to make a query and talk to the large
8:31 language model, but also to get access to resources that we have.
8:36 If I take a look at the folders that I have, I can see here
8:39 that we are reading resources at this URI,
8:43 and here I have access to a folder called computers.
8:46 That's because in a previous search I looked for computers.
8:50 Let's go get access to those papers.
8:52 And here we'll see. I have the information right up here.
8:54 Instead of writing a tool to go ahead and fetch that data and requiring
8:59 that the model does all that work,
9:00 I now can provide this context to the model
9:03 and if the model chooses to go ahead and add it to its context window,
9:07 and the application requires
9:09 so, I can make use of that.
9:11 Let's go take a look at the prompts that I have.
9:13 Remember,
9:14 there's a prompt that we made on the server called Generate Search Prompt.
9:17 And we can actually see that the fetch server as well provides
9:21 a prompt for fetching an URL and extracting its contents as markdown.
9:25 The argument here is the URL.
9:27 Let's go ahead and make use of this prompt.
9:29 The way to do so is to add the slash prompt command.
9:32 And we'll see here that the usage requires the name of the prompt,
9:36 as well as any arguments that are required.
9:38 So let's go ahead and use our generate search prompt.
9:42 I'll use the slash prompt command.
9:44 I'll pass in the name of our prompt.
9:46 And then I'll
9:46 go ahead and pass in the argument that is required which is the topic.
9:50 Let's go ahead and search for some papers on that.
9:53 The NUM papers is optional.
9:55 So I can pass in a number if I want.
9:57 Or I can just default to five.
9:59 So let's go ahead and use this prompt with the dynamic variable of topic
10:03 that I've defined.
10:04 We'll see here, we're processing that to get the prompt.
10:08 And then we're generating the text necessary and executing that prompt.
10:12 We'll see here, this is going to look familiar,
10:14 we're talking to arxiv to get access to those particular papers.
10:18 We're going to take those papers
10:20 and we're going to add them to the folder that we have for math.
10:24 Once this is done, I should also be able to access this data via a resource.
10:29 Remember that those resources are updated dynamically as data
10:32 changes in my application.
10:34 My query is finished and we can see the response that the model is giving me.
10:38 Let's go take a look at what our folders look like.
10:40 And we can see here,
10:41 we now have topics for computers and math.
10:44 And if we want to access that file, we can go ahead and take a look at
10:47 what's there.
10:48 We're making use of prompts and resources together.
10:51 In this lesson, we've done quite a bit.
10:53 We've explored how to add prompts and resources on the server
10:57 and then consume them in our chatbot.
10:59 We put together some of the core primitives like tools,
11:03 resources, and prompts connecting to multiple MCP servers.
11:07 In the next lesson, we're going to start introducing other kinds of hosts
11:10 for more powerful interfaces.
11:12 But with many of these ideas that we've seen before.
11:14 As always, if you want to hop out, type in quit,
11:17 and I'll see you in the next lesson.
```
# 8
```

0:02 The MCP server you built and the reference servers you used
0:05 can also be used with any other MCP compliant application.
0:10 That could be an IDE for a desktop app or an agentic framework.
0:14 Claude desktop is an example of such an application.
0:17 In this lesson,
0:18 you'll learn how to configure Claude Desktop to connect to MCP servers.
0:22 Let's have some fun!
0:23 So you previously seen how to build your own MCP servers and clients and use
0:28 prompts, resources and tools to build pretty sophisticated applications.
0:32 And I mentioned that some of that code, particularly when building
0:35 clients and multiple clients, can be a bit low level.
0:38 What you're going to see is the ability to use applications like Claude desktop
0:42 and a variety of other agentic products, to use MCP to connect to MCP servers
0:48 and abstract away some of the challenges of that lower-level coding.
0:51 Right here I'm in a folder called MCP project on the desktop,
0:54 and I have the research server that you made in a previous lesson.
0:57 What I'm going to do now is follow the same steps to create
1:01 an environment with uv and install the necessary dependencies.
1:04 And then I'm going to navigate to Claude Desktop and bring in this MCP
1:08 server alongside a couple other other reference servers that we've seen.
1:12 So we'll start by doing uv init.
1:13 I'll make sure to uv venv and I'll activate my virtual environment.
1:17 I'll install the necessary dependencies that we have.
1:20 So I'll go ahead and add arxiv as well as MCP.
1:24 Once I've
1:24 got these added, I have all of the necessary dependencies
1:28 that I need to start my server, but I'm not going to run the server here.
1:32 I'm going to use Claude Desktop to do that for me.
1:35 The same way that we set up our own Json file for configuring
1:38 how to connect to MCP servers,
1:40 we're going to do something similar with Claude Desktop. In Claude Desktop,
1:43 I'll head over to settings.
1:46 I'll go to developer.
1:48 And we can see here
1:49 that I can edit a config file to start connecting to MCP servers.
1:53 I'm going to go ahead and open up this JSON file.
1:56 And inside here, I'm going to paste in our configuration
1:59 file from before, but with a slight change.
2:02 We're still referencing the name of the server and the command necessary
2:06 to get the server started.
2:07 What's important though, is that for our research server, I'm
2:11 specifying the exact file path to go ahead and run the server.
2:20 If you
2:21 remember, when using standard IO and connecting locally,
2:24 the client is going to start a connection and a subprocess to these servers.
2:28 With this file and the Claude desktop AI application.
2:32 This is all abstracted away from us.
2:34 So all of that lower-level code that we were doing earlier
2:37 we don't need to worry about.
2:38 It's important to know how it works under the hood.
2:40 But what we're doing here is that same idea
2:43 multiple clients connecting to multiple servers.
2:46 But all we need is the configuration file.
2:48 You're also going to see in Claude desktop, what the interface looks like
2:52 when working with prompts and tools and resources.
2:55 Once we've modified this file, we have to close Claude desktop
2:58 and reopen it so we can establish these connections.
3:01 Now that I've restarted Claude Desktop, I can take a look at some of the tools
3:05 I have access to.
3:06 And we saw this before when exploring SQLite 3.
3:09 I have my local research server with the tools that we've defined.
3:13 I have my fetch and file system and I have from the research server
3:18 my resource as well as my prompts. The interface for prompts and resources
3:22 and tools is completely up to the developer of a tool like Claude Desktop.
3:27 There are a variety of AI assistants and tools
3:30 and agentic products that you can use that support the Model Context Protocol.
3:34 Let's examine those. Over here on the Model Context Protocol documentation,
3:38 we can see a list of applications that support MCP integrations.
3:42 And a lot of this should look relatively familiar.
3:44 These are some of the primitives we've explored,
3:46 like resources, prompts, and tools.
3:48 And in the last section,
3:49 you'll learn a bit about sampling and routes other powerful primitives
3:53 that are on their way to getting more and more adoption.
3:55 As you can see here, there's quite a range of applications
3:58 from web applications, agentic applications, command line interfaces,
4:02 integrated development environments that support the model Context Protocol.
4:06 You can click on any of these, discover them, and see
4:09 how to start talking to MCP servers within these existing applications.
4:13 What's so powerful about this idea
4:15 is that while this may look extremely vast,
4:17 you've actually seen already how a lot of this works under the hood
4:20 by building your own servers and own clients.
4:23 So let's take a look at how Claude desktop users are
4:26 tools, prompts, and resources by connecting to MCP servers.
4:30 Now let's put this all together.
4:31 I'm going to paste in a prompt that I have here to use the fetch tool
4:35 to visit DeepLearning.AI
4:36 I can find an interesting topic about machine learning.
4:39 I'll then use our research server to research a few papers
4:43 and summarize the main topics covered.
4:45 I'll then use the artifacts feature to generate a web-based quiz
4:48 application with a set of flashcards based on the key topics in the papers.
4:52 So what we're doing here is combining a set of tools
4:55 from different MCP servers to fetch the data that we're looking for.
5:00 So we're using the fetch tool here to visit DeepLearning.AI.
5:03 And here, looks like there are quite a few interesting topics.
5:06 One of those being multi-modal LLMs and advancements in AI models.
5:10 We'll then use our search papers tool from the research server
5:14 to extract some information about those papers.
5:16 We'll even look at one more recent paper to ensure we have a good sample.
5:24 Here, we see the two papers that have been discovered
5:26 and a web application that's being built at the moment.
5:29 The ability to incorporate visualizations and tools like artifacts
5:33 with the tools that we get from MCP servers, allow us to build really powerful
5:37 applications across a variety of domains with relative ease.
5:41 While we just have a small example here, you can let your imagination
5:43 carry you again for all the different use cases we have.
5:47 We've got a summarization and a little game right here.
5:49 We can take a look at what some of these answers are.
5:51 And with a little bit of prompting,
5:52 we can always make this look a little bit nicer.
5:55 In this lesson, you've seen how to use tools like Claude Desktop
5:58 to connect to MCP servers and abstract away a lot of the lower
6:02 level networking and code.
6:03 You've also seen a list of many different MCP
6:06 compatible applications to get you up and running with different IDE's,
6:11 web applications, and client-side or desktop applications.
6:14 In the next section,
6:15 we'll dive a little bit deeper into building remote MCP servers.
6:19 See you then.
```
# 9

```
0:02 Your AI application can also connect.
0:04 To MCP servers running remotely.
0:07 In this lesson, you'll learn how to build a remote server,
0:10 test it, and then deploy it.
0:12 Let's jump in.
0:13 So far we've seen how to build servers, build clients and hosts,
0:18 but it's all been done locally using standard IO.
0:21 Now let's see what it looks like to create and deploy a remote server.
0:25 The good news is
0:26 we don't actually have to change that much. Inside of the research server
0:30 I've made a small configuration for our port to allow connecting
0:33 a little bit easier. Everything else is going to feel the same.
0:36 We still have our tools.
0:38 We still have our resources.
0:39 We still have our prompts.
0:41 But where things are going to look a little bit
0:42 different, is at the very bottom here, where we specify the transport
0:46 that we're using. At the time of this recording,
0:49 the SDK in Python don't yet support HTTP streamable.
0:53 So we're going to be using SSE.
0:55 The good news is once those SDKs are released,
0:57 it should be a very quick change to modify the transport.
1:01 Now that we've seen how to modify our research server to get up and running
1:05 with SSE, let's go ahead and connect to it with the inspector.
1:09 From this code that we provided, you can see that the server
1:11 is already running at this particular URL.
1:15 So there's nothing you need to do here to get the server up and running.
1:18 Now that the server is running, let's bring in the terminal
1:21 and connect to that server using the Inspector.
1:29 So I'll run npx @ Model Context Protocol
1:33 slash inspector.
1:37 This will get the inspector started.
1:39 And now I can go ahead and visit this in the browser.
1:41 Over here in the inspector,
1:43 I'm going to make sure that I have the proxy address that we had before.
1:47 And you can find this in the notebook as well.
1:50 And then I'm going to go ahead and make sure that my transport type is SSE.
1:54 I'm now going to put in the URL to connect via SSE.
1:57 And we provided this to you as well.
1:59 Let's go ahead and connect.
2:01 We can see here that we've initialized our connection.
2:03 And we have our resources, prompts, tools, and other primitives.
2:07 I can list my resources and take a look at all of my folders.
2:10 I can list my templates to get access to a particular topic.
2:13 So let's go see if we can search for math like we had before.
2:17 Go ahead and run that search. For the prompts,
2:20 We have our prompts available. And for tools,
2:23 I have my tools available as well.
2:25 Now that we're successfully connecting to this server using SSE,
2:29 let's see how we can deploy this so that anyone can access our MCP server.
2:34 Let's go ahead and deploy this server using a tool called render.
2:38 I'll go bring in my terminal here.
2:39 And right now I'm inside of the MCP projects folder.
2:43 In order to work with render, I need to make sure
2:46 that I have a git repository and then push that to GitHub
2:49 so that render can access that particular repository.
2:52 If you're not familiar with git and GitHub,
2:54 I'll walk you through all those commands and what they do.
2:57 If you're familiar with git,
2:59 this should be a breeze. To get started with git,
3:01 the first thing I'm going to do is initialize an empty git repository.
3:05 I'm then going to make sure
3:06 that there are some files and folders that are not included in git.
3:10 So I'm going to go ahead and add the string then
3:13 to a file called dot git ignore.
3:15 This is going to make sure
3:17 that when I add and commit files the dot then folder is not included.
3:21 If I take a look at my git status right now, I can see that
3:24 I don't see that dot env folder, which is great.
3:27 The next thing I need to do here, is make sure that
3:29 my dependencies are compatible with render.
3:32 Since render does not support uv at the moment.
3:35 We're going to go ahead and make sure that we use Pip instead
3:39 for dependency management.
3:40 We're going to have to take the dependencies
3:42 for our server that we set up with uv and make them compatible with Pip.
3:46 The command we want is uv pip compile.
3:48 So we're going to go ahead and type in uv pip compile our pyproject .toml
3:54 And what you see here is the output
3:57 of turning our dependencies into what Pip needs.
4:01 So we're going to take the result of that.
4:03 I'm going to send it to a file called requirements.txt
4:06 Now that we have that file called requirements.txt,
4:09 there's one more file that we need to make sure
4:11 we have the right version of Python for render to use.
4:14 We're going to go ahead and send the string Python 3 11 11
4:20 into a file called runtime dot txt.
4:23 Now that we have the necessary files, let's take a look at git status
4:27 and we can see that now we have requirements.txt as well as runtime txt.
4:31 Let's go ahead and add and commit.
4:35 And here our commit message is going to be ready for deployment.
4:40 And now we need to take this code on git
4:43 locally and bring it into a repository on GitHub.
4:47 We need a GitHub repository so that render can pick up and load the project files.
4:52 So in the browser I'm going to head over to github.com
4:54 slash new to make a new remote repository.
4:58 And I'll give this repository a name.
5:00 Let's call this remote research.
5:04 I'll go ahead and create that repository.
5:07 And once I've created that repository
5:09 I'm just going to use these last couple steps.
5:12 In order to push an existing repository from the command line,
5:15 I need to know where I'm sending that to.
5:18 So I'm going to go ahead and copy and paste this command
5:21 and go back to the terminal to run it.
5:24 If I've done that successfully, I should be able to type in git remote,
5:27 dash B and C origin as well as what I put in.
5:32 I'm now going to go ahead and push this code up to GitHub,
5:35 so I'll type in git push origin main. If this has been done
5:39 successfully, I should be able to go back to the browser.
5:46 Refresh the page and see that my code is there.
5:50 Now that this code is successfully on GitHub,
5:52 let's go ahead and make sure that render can pull it in
5:56 so we can deploy successfully.
5:57 I'm going to head over to render.com
6:00 And at render.com I'll make sure I sign up.
6:03 Once you've signed up or signed in you can head over to your dashboard.
6:07 Once I sign in to the dashboard, I can now add a new service.
6:11 So I'll click on Deploy Web Service.
6:13 Or I can always go to new and have that there.
6:16 If I authenticate through GitHub I can put in repositories that I have.
6:19 Or I can put in a public git repository if I just want to put in the URL.
6:24 I'll go ahead and select remote research.
6:26 And the only thing I need to change
6:28 here is the command that we use to start our server.
6:32 The command to do so is Python research_server.py.
6:38 I'll make sure right now I'm using the free plan.
6:41 And once this is done, I'm ready to deploy.
6:43 This might take a little bit
6:44 so give it a second while it starts the application.
6:47 And as we start deploying we're going to see the commands necessary
6:50 to get our application up and running.
6:52 Things that we actually saw before when we were running this locally.
6:56 So we can see here we're using that Python 3 11 11
6:59 that we specified in our runtime txt
7:02 We're installing the dependencies from our requirements.txt file.
7:05 Since we're using pip here for dependency management. Once
7:08 this is deployed, when we visit this link we're not going to find anything.
7:12 So we should expect a 404 error.
7:14 But we do want to see is if we go to slash SSE
7:17 we're seeing a response with the session ID back from the server.
7:21 We're running the command Python research_server.py.
7:24 So let's go ahead and see what things look like.
7:26 The first time you look at this,
7:27 it might take a second as your application is loading.
7:30 So feel free to give it a minute or two.
7:32 And we'll refresh the page. And as we refresh the page,
7:35 and as expected I'm getting a 404.
7:38 If I head over to the SSE endpoint, I can now see that I made it
7:43 with this particular endpoint and a session ID.
7:46 This means that I've deployed successfully.
7:48 If you want to try this out in the Inspector or
7:51 in other tools, take a look at the resources we have.
7:54 Great job getting this deployed and I'll see you in the next lesson.
```
# 10
```
0:01 In this course, you've learned about the core concepts of MCP.
0:05 You've built a server that exposes tools, resources, and prompts,
0:09 developed a chatbot, that can connect to multiple servers,
0:13 use Claude Desktop to build more sophisticated
0:15 applications and deploy your own remote server.
0:19 Congratulations!
0:20 MCP is constantly evolving.
0:22 In this last lesson, you'll learn about other features of MCP
0:25 and some of the exciting things coming soon to the protocol.
0:29 I'll see you there.
0:30 There's a lot you've learned about the Model Context Protocol.
0:33 You've learned about hosts and clients and servers, tools, resources and prompts.
0:38 And then you've got a chance
0:39 to write some code to power larger applications using all of these ideas.
0:44 But there's still a bit that we have not yet covered
0:46 about the model context protocol.
0:47 Much of this is an active development, and you can always examine the latest
0:51 on the specification, run through GitHub
0:53 and through the discussions that you can find.
0:55 The first piece
0:56 we haven't covered yet is authentication in the Model Context Protocol.
1:00 In the March specification update, OAUTH 2.1 was added
1:04 as the means of authentication with remote servers.
1:07 What this allows for is for clients and servers
1:10 to authenticate and send authenticated requests to data sources.
1:13 You can imagine many different servers need to access data
1:17 that requires some form of authentication.
1:19 This requires the client making a request to the server.
1:22 The server then requiring some user to authenticate.
1:26 And once the authentication process is successfully done,
1:29 the client and server can exchange a token and the client can make authenticated
1:34 requests to the server and then to the data source.
1:37 This part of the protocol is an active development,
1:39 and there are always newer features and security pieces being added.
1:43 But authentication will primarily be done with the OAUTH 2.1
1:46 protocol. To highlight that in some depth,
1:49 this is an optional feature of the Model Context Protocol,
1:52 but it is highly recommended for remote servers. With standard IO,
1:55 we use environment variables and don't have the need
1:58 for this kind of authentication.
2:00 This is built on established standards that you can take a look at with
2:03 these links below.
2:04 So while we've explored primitives that to be exposed by the server
2:09 tools, resources and prompts, we also have primitives that the clients can expose.
2:14 These include roots and sampling.
2:16 Let's dive into this.
2:17 A route is a URI that a client suggests a server should operate in.
2:22 The idea is centered around only looking in specific folders
2:26 for files that you might need. When the client connects to a server,
2:30 it can declare the roots that the server should work with.
2:33 This can be useful for filesystem paths,
2:35 but can be any valid URI, including HTTP URLs.
2:39 The benefits of roots allow for security limitations.
2:42 Allow for keeping the server focused on a relevant file path or location,
2:47 and roots also have some versatility baked in, where again,
2:50 they're useful for file paths, but also can be any valid HTTP URI.
2:54 We're slowly starting to see more and more clients adapt this primitive,
2:57 and it's an important one to keep track of as the protocol evolves.
3:00 The last primitive that we're going to cover is sampling.
3:03 Sampling allows for servers to request inference from a large language model.
3:09 Kind of like the other side of communication,
3:11 where instead of the client talking to the large language model,
3:14 the server can talk back to the client and request inference.
3:17 An example here might be a situation
3:19 where your users report that a website is slow for some reason.
3:23 Your MCP server can then collect server logs, performance metrics, error logs,
3:28 and communicate with a variety of data sources to see what's going on.
3:32 Instead of the server handing that back to the client,
3:35 putting everything in the context window,
3:37 or potentially any kind of breach of security
3:40 between the server and the client, the server instead can talk directly
3:43 to the large language model and ask it to diagnose performance issues.
3:47 The large language model analyzes the patterns and returns data back,
3:51 and then the server can generate the steps to make the website a bit less slow.
3:55 When there are concerns from a security standpoint or breaching boundaries.
4:00 Or you might not want all that data coming back to be put into context.
4:04 Sampling and creating sampling loops,
4:06 it's a very powerful way for servers to request inference and kind of switch
4:11 the direction of communication from what we've seen before.
4:14 This is also quite powerful
4:16 as we explore agentic capabilities with the Model Context Protocol.
4:19 As we start to move towards
4:20 a world where more models are talking to different data sources,
4:24 and we're giving more autonomy to models to call different tools
4:28 and go off on their own,
4:29 we believe MCP will be a foundational protocol for agents.
4:33 As we start to think about how MCP can be used with agentic capabilities,
4:38 you can imagine a scenario where an user and a large language model
4:41 need to access a variety of MCP servers.
4:45 What's so powerful about the Model Context Protocol is
4:48 that there's a composable and somewhat recursive nature,
4:51 where clients can be servers, and servers can be clients.
4:54 What this allows us to do is to start creating an architecture
4:58 where we can take advantage of the ability for clients to communicate with servers,
5:02 but also for servers
5:03 to request the data that they need through sampling back to a client.
5:08 What we've set up here is the idea of a multi-agent architecture,
5:11 where the application and a large language model communicate with an agent.
5:16 This agent happens to be an MCP client and a server,
5:20 and it can serve data back to the application,
5:23 but it can also connect to other clients
5:25 and servers through the Model Context Protocol.
5:29 You can imagine that we have agents for analysis, for coding,
5:32 for research that also happen to be MCP servers.
5:35 And if they need to connect to other servers, clients as well.
5:39 Through this composable nature,
5:40 we can start to think about architectures that allow for multiple agents
5:44 all speaking the same language with the same protocol.
5:47 The next large piece that's on the roadmap
5:49 for the model Context Protocol, is the idea around a unified registry.
5:53 The purpose of this is to standardize the way
5:56 in which we think about discovering servers themselves.
5:59 As we've seen before,
6:00 there's a lot of excitement in the open source community,
6:03 and there are many different servers for data providers.
6:05 Tools like Google Drive, GitHub and so on may have dozens of MCP servers,
6:10 but just like with packages with NPM or PyPI, there's an opportunity
6:14 for malicious code to exist inside of these particular servers.
6:18 So the registry API serves the purpose of discovering servers,
6:22 of centralizing where these servers live, and also verifying that these servers
6:27 have been trusted by the community and by companies themselves.
6:31 This also allows for versioning of particular MCP servers
6:35 to lock in dependencies, just like you would in your application.
6:38 What this also gets exciting is the ability for MCP servers
6:42 to let agents self discover them.
6:44 You can imagine a scenario
6:45 where a user needs to fix a bug based on something in some logs.
6:49 The agent then searches through the registry API
6:52 for the official MCP server, installs it and queries and suggests the fix.
6:58 In this particular use case, instead of requiring the application to be connected
7:03 to a variety of servers from the start, we can start to build applications
7:07 where MCP servers are dynamically discovered and connected to.
7:11 As we think about layering this on with authentication,
7:14 we can imagine that a user has a request that requires a server to be discovered.
7:18 Similar to other protocols like OAUTH and the agent-to-agent protocol
7:23 that Google recently announced,
7:24 the idea of putting in a JSON file in a well-known folder is something
7:28 that's been done before, except here in this MCP JSON,
7:31 we specify the endpoint of a server to connect to, the capabilities
7:36 or primitives that it exposes, and then the authentication that's required.
7:39 So a user might ask how we manage my store on Shopify.
7:42 The agent or AI application
7:45 will see if Shopify has a well known MCP JSON file.
7:49 And if it does, it will figure out
7:51 what endpoint to connect to and what authentication is required.
7:54 Once the user authenticates,
7:56 the agent can perform the necessary action. Through a registry API,
8:00 we can allow for this idea of dynamic discovery and through layering on Oauth2,
8:05 we can ensure that these connections are secure.
8:08 As you might see in the suggestions, and in the discussions, there's a lot more
8:11 that's coming to the protocol.
8:13 As more and more clients support HTTP streamable.
8:17 The aim is to achieve a smooth transition between staple and stateless capabilities.
8:21 As remote MCP servers continue to be developed,
8:24 it's important to expand the ecosystem to support even more and more of those.
8:28 As you can imagine, when multiple MCP servers are being used,
8:32 it's very possible for tools to have naming conflicts.
8:35 You can imagine servers
8:36 that might have generic names of tools like fetch users, fetch entities,
8:40 and the model might get confused about what needs to be fetched.
8:43 So it's important to think about preventing collisions
8:45 and creating logical groups of servers or tools.
8:48 We spoke a bit about sampling or proactively requesting context.
8:52 There's a lot of work that's being put into the protocol and it's conversations
8:55 we have to enable primitives like sampling to become much more popular.
8:59 And finally, while OAUTH2 is relatively new to the specification,
9:03 there's still quite a bit more to think
9:04 about with regards to authentication and authorization at scale.
9:08 In just a short amount of time,
9:09 you've seen so much about the Model Context Protocol.
9:12 You learn conceptually about the primitives. You've built, servers
9:15 and clients and hosts, and you've seen how to deploy remote MCP servers.
9:20 There's still so much more to be discovered.
9:22 So I encourage all of you to take a look at the discussions and the conversations,
9:26 and keep building and researching as much as you can.
9:29 Thank you so much for joining me in this journey,
9:31 and I can't wait to see what you build with MCP.
```
<img width="923" height="471" alt="image" src="https://github.com/user-attachments/assets/e6bebfaa-3a87-4f20-986b-9407af76e9f6" />

<img width="857" height="379" alt="image" src="https://github.com/user-attachments/assets/e5b9303d-107e-44f0-841e-00ba45c3f138" />

<img width="806" height="365" alt="image" src="https://github.com/user-attachments/assets/0257974c-ab0f-4b3b-8706-8a006cc5a41d" />


<img width="871" height="399" alt="image" src="https://github.com/user-attachments/assets/aa6071cb-8b53-4e1e-abe6-9b5a29167c1a" />

<img width="859" height="488" alt="image" src="https://github.com/user-attachments/assets/fa6c5e55-245b-4178-b1b9-4d8c8e7974e5" />

<img width="970" height="419" alt="image" src="https://github.com/user-attachments/assets/641f1a82-9296-4588-8e02-2362f5c93f6d" />

<img width="946" height="476" alt="image" src="https://github.com/user-attachments/assets/ac4027a2-cc93-44d5-8c60-8cc875f62b78" />

<img width="935" height="463" alt="image" src="https://github.com/user-attachments/assets/f1cb438e-414b-4deb-b8c6-aed7134cfbf3" />

<img width="858" height="427" alt="image" src="https://github.com/user-attachments/assets/66590f9e-15ff-4187-b641-d814bfad575d" />



