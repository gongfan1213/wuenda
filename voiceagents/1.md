# 1
```
Transcript
0:02 Welcome to Building AI Voice Agents for Production
0:05 taught by Russ d'Sa (Sah), Shayne Parmelee and Nedelina Teneva.
0:10 Russ is the co-founder and CEO and Shane a developer advocate at LiveKit.
0:15 And Nedelina is head of AI, at RealAvatar and developed a
0:19 conversational avatar together with the DeepLearning.AI team.
0:23 RealAvatar is also a portfolio company of AI Fund, which I lead.
0:27 I'm excited about agents who can converse with users.
0:30 This is turning out to be an important way for people to interact with AI agents.
0:35 DeepLearning.AI
0:35 and RealAvatar's teams, including Nedelina,
0:39 had a great experience building a conversational avatar using LiveKit.
0:44 I personally also really enjoyed using LiveKit for various other projects.
0:48 In this course, we want to share with you
0:50 some best practices for building voice agents.
0:54 Let me describe our project, which we'll use as a running example.
0:57 We started with a conversational agent, similar to many of the projects
1:01 you may have seen or built in previous short courses.
1:04 We developed an agentic workflow to get the system to try to choose words
1:08 to try to say things similar to what to what I would say in different circumstances.
1:12 We then added on the input side, a speech to text model to convert the user's
1:17 audio speech to text for the agentic workflow to process. And then on the output side,
1:21 added a text-to-speech model to take the text output
1:25 and turned that into speech.
1:27 That can then be read out to the user using a model
1:30 from ElevenLabs for the audio generation.
1:33 The model was trained to sound like me, and I think the audio turned out
1:37 pretty decent. You hear later and you can decide.
1:40 We wanted this to scale to a large number of users,
1:44 and so we moved to a cloud infrastructure
1:46 that could support many simultaneous users.
1:48 Finally, users of this service can be anywhere in the world.
1:52 This introduced real-time networking concerns
1:55 and audio integration issues.
1:58 Our solution, use cloud resources to support the front end of the avatar,
2:03 with an agentic workflow on the back end
2:05 and we integrate to a LiveKit to provide communication infrastructure.
2:09 Now Nedelina and Russ will tell you more about this in the course.
2:13 In the first lesson, you'll learn the components
2:15 of a voice pipeline, including speech to text and text-to-speech models,
2:19 as well as voice activity and end of turn detection.
2:22 You'll also learn how important latency is
2:25 and some strategies for keeping latency low.
2:28 Then we'll try out a voice agent.
2:30 You'll learn how voice agents are really different from other applications.
2:34 Voice agents have state and to be effective, must have a presence,
2:39 just as if there was another person on the other end of the conversation.
2:43 In lesson four and five, you'll build a voice agent
2:46 that you can use in the course or download to your own machine.
2:49 You'll learn to measure latency
2:50 in a voice pipeline to achieve natural conversation.
2:53 Many people have worked to create this course.
2:56 I'd like to thank from LiveKit Theo Monnom and from DeepLearning.AI, Geoff Ladwig.
3:01 Additionally, I'd like to thank Thor Schaeff from ElevenLabs
3:05 who created the speech-to-text model you'll be using in this course,
3:09 and to arrange the support for this course.
3:11 Thank you Andrew.
3:12 It's great to be a part of this course.
3:15 I hope you will find conversational AI agents as compelling as we do,
3:20 and will take the time to not just explore text-to-speech with ElevenLabs,
3:25 but also our fully fledged conversational AI platform,
3:29 which allows you to add voice to your agents within minutes.
3:34 We can't wait to see what you will build.
3:36 That sounds great.
3:37 Let's get started with the next lesson an overview of voice agents.
```

# 2
Transcript
```
0:02 In this lesson, you'll dive into the fundamentals of AI voice agents
0:05 and their key components
0:06 such as speech to text, text to speech, and large language models.
0:10 While analyzing the latency introduced at each layer of the voice agent stack.
0:15 You'll explore how platforms like LiveKit mitigate these latency challenges
0:18 by providing optimized network infrastructure and implementing low
0:22 latency communication protocols.
0:24 Finally, you walk through a minimal example of building
0:27 a voice agent in Python and touch on practical approaches
0:31 for evaluating and improving voice agent performance.
0:34 Let's get started.
0:35 Before we dive in, you might be wondering what exactly is an AI voice agent?
0:40 Simply put, an AI voice agent brings together speech capabilities
0:44 and the reasoning power of foundation models to enable real-time human
0:48 like conversations.
0:49 Voice agents are useful in a wide range of scenarios. In education,
0:54 they can guide personalized skill development or conduct mock interviews.
0:59 In business, they can help handle customer service
1:01 calls by booking a table at a restaurant or assisting with a sale.
1:06 And because voice
1:07 agent interactions are hands-free, they can also enhance accessibility.
1:11 Think of a patient using a voice
1:13 agent to log symptoms or practice talk therapy at home.
1:17 Now let's break down the anatomy of a voice agent stack.
1:21 The system takes as input the user's voice
1:23 like a user question or request, and produces a voice response.
1:27 In some cases, that audio output might also be synchronized with video,
1:31 such as a talking head avatar.
1:33 But for this course, we'll focus on audio only interactions.
1:37 When designing the internals of a voice agent, we have two main options.
1:41 The first is to use a speech-to-speech or real-time API.
1:45 This option is simpler to implement, but it offers
1:48 less flexibility and control over the agent's behavior.
1:52 The second and the approach will primarily focus on
1:54 in this course, is the pipeline approach.
1:58 The voice agent pipeline is made up of three components
2:01 a speech-to-text model or API.
2:04 An LLM or agentic framework, and a text-to-speech model or API
2:09 that generates the final audio output.
2:12 Let's take a more detailed look at each component of the voice agent
2:16 pipeline.
2:16 The first component is Automatic Speech Recognition, or ASR,
2:20 also referred to as speech-to-text or STT.
2:24 This involves transcribing a given audio signal, typically a waveform, into text.
2:29 The input is raw audio and the output is the corresponding transcription.
2:33 The second component is the large language model or a broader agentic workflow,
2:38 which generates a response based on the transcribed text.
2:42 This layer may involve one or more LLM agents, often
2:45 enhanced with tool use, memory or planning abilities.
2:49 As an aside, a voice agent can also produce transcripts
2:52 enriched with supporting materials such as images or links
2:57 as a byproduct of its functionality. This pipeline
2:59 can be generalized to support such multimodal LLM responses,
3:03 enabling the display of both spoken output and visual context when needed.
3:08 The third component is text-to-speech or
3:11 TTS, also known as speech synthesis.
3:14 This is the task of converting the generated text back into natural sounding
3:19 intelligible speech.
3:20 The input here is text and the output is audio.
3:23 In a demo you'll see shortly, the synthesized voice is that of Andrew.
3:27 Beyond these three main components, it is important to highlight two
3:31 additional tasks that are essential for correctly processing
3:34 human speech and occur before the ASR step.
3:37 The first is Voice Activity Detection, or VAD,
3:41 which determines whether human speech is present in the audio signal.
3:45 For example, a lack of detected speech may correspond to unnatural pause
3:49 or section sections dominated by background noise.
3:52 The second task is end-of-turn detection, which identifies
3:56 when a speaker has completed their turn in the conversation.
3:59 This is a non-trivial challenge, as speech often includes pauses of varying lengths
4:04 depending on the speaker's language, habits, and expressive style.
4:09 Now that we've reviewed the core components of the two voice agent
4:12 architectures, the next question is how do we actually build these components?
4:17 Fortunately, we don't need to start from scratch.
4:20 Instead, we can focus on the parts of the stack
4:23 that matter most for our specific use case.
4:26 For instance, depending on the application,
4:28 some components may require more attention than others.
4:32 If you're developing a voice agent for a clinical setting,
4:35 for example, the ASR component becomes critical.
4:38 You'll need to accurately recognize specialized medical vocabulary
4:42 and meet strict precision requirements.
4:44 On the other hand, if you're working on a restaurant booking agent, the LLM
4:48 or agentic workflow becomes more important as you'll need robust reasoning
4:52 and reliable tool use to avoid issues like overbooking tables.
4:56 Unless your use case requires a specialized or on-device model,
5:00 you can choose from a wide range of providers for TTS, STT, and LLM APIs.
5:05 On this slide, we've listed some of those options in the gray boxes,
5:09 and as you can see, many providers are available and worth exploring. In the demo,
5:14 at the end of the lesson, we use OpenAI for STT and ElevenLabs for TTS.
5:19 For a text-to-speech output, we trained an ElevenLabs
5:22 custom voice model using a recording of Andrew's voice
5:26 to create a consistent and personalized AI voice call for speech synthesis.
5:30 Finally, for the LLM component, if low latency is a key requirement,
5:35 you may want to explore open source Llama models served by fast
5:38 inference providers such as Groq, Cerebras, or TogetherAI.
5:43 If you choose to build your voice
5:44 agent using the speech-to-speech or real real-time API approach,
5:48 there are also several providers available to support that workflow.
5:52 These APIs abstract away much of the underlying pipeline,
5:56 and are ideal for use cases where rapid deployment
5:59 is more important than fine grained control.
6:02 Regardless of which agent architecture you choose, you'll face one major challenge: timing.
6:06 Humans expected responses within a narrow window,
6:10 and if the system lags, the interactions quickly feel unnatural.
6:14 To maintain a smooth conversational flow,
6:17 your infrastructure must support low-latency audio streaming
6:21 and efficiently manage input and output streams.
6:24 In the pipeline approach, for example, you'll need to orchestrate real-time
6:27 interactions across multiple users while ensuring seamless transitions
6:32 between components like VAD, STT, the LLM,
6:36 and TTS, without introducing noticeable delays at any stage.
6:41 When considering latency, it is helpful to start with a baseline-
6:44 how quickly do humans expect a response in natural conversation?
6:48 User studies have shown that, on average, people anticipate a response within 236
6:53 milliseconds after their conversation partner finishes speaking.
6:57 However, the standard deviation is quite high around 520 milliseconds,
7:03 which reflects the natural variability in human speech.
7:06 It's also important to note that these numbers are based on English speakers.
7:10 Other languages can exhibit significant faster or slower response times.
7:15 Now, if you look at the table on the slide, we can see the latency
7:18 introduced by each step in the voice agent pipeline.
7:22 In the best-case scenario, with efficient input output stream handling, the lower
7:26 bound for full voice agent response is approximately 540 milliseconds.
7:31 This places a just within one standard deviation of human expectations.
7:36 However, depending on the service level agreements of the providers you use,
7:40 the latency can increase
7:42 to over a second and a half, which users will almost certainly notice.
7:46 So how can we approach the lower bounds of latency
7:49 that align with natural human conversation?
7:51 The key lies in real-time peer-to-peer communication,
7:54 which enables direct data exchange between devices,
7:58 bypassing intermediary servers and significantly reducing delays.
8:02 In this set up, your client, such as a web browser or mobile
8:06 device, acts as one peer, while your voice agent back end functions as the other.
8:11 LiveKit's infrastructure is designed to support this with a globally
8:14 distributed mesh network for media forwarding.
8:18 At the core of this system are several technologies.
8:20 First, WebRTC, or Web Real-Time communication
8:24 is an open-source project that provides web and mobile applications
8:28 with real-time communication capabilities through standardized APIs.
8:33 Second, WebSocket is used to establish a client-server
8:36 handshake, enabling efficient signaling and session management.
8:41 Finally, LiveKit's open-source
8:42 implementation relies on asynchronous processing and careful management
8:47 of input/output streams and streaming APIs,
8:50 particularly for the STT,TTS and LLM components.
8:55 This ensures smooth, low-latency performance throughout the voice
8:58 agent pipeline.
9:00 While the underlying peer-to-peer infrastructure is complex
9:03 and something Russ will walk through later in the course,
9:06 LiveKit abstracts much of that complexity and makes it remarkably simple
9:10 to define an AI voice agents with just a few lines of code.
9:14 On this slide,
9:15 you can see a minimal example of how to set up a voice agent back end.
9:18 There are three main components to focus on.
9:21 First, defining the agent itself, including any prompts.
9:25 Second, the agent session, which links together your chosen speech
9:29 to text, LLM and text to speech providers into a functional pipeline.
9:34 And third, the entrypoint() function, which is executed
9:37 as the main function for each new peer to peer communication.
9:41 You'll dive deeper into the code and configurations later in the course.
9:44 For now, note that in the demo of this lesson, we said the TTS voice
9:49 ID variable in the code to reference a custom ElevenLabs voice clone.
9:54 We've trained a real avatar. To wrap up this section, I want to highlight
9:58 a few unique challenges that come with building voice based applications.
10:03 First, speech disfluencies, such as filler words like "um" or long
10:07 pauses, can introduce artifacts in transcription and affect
10:10 end of turn detection.
10:12 These issues then propagate into the input given
10:15 to the LLM, potentially reducing output quality.
10:18 Second, if you're working on multilingual voice agents, keep in mind
10:22 that multilingual ASR models generally underperform compared to English ASR.
10:28 Now let's turn briefly to latency optimization.
10:31 Accurately measuring latency in practice is challenging,
10:35 especially when trying to separate client side delays from server-side delays.
10:39 To help minimize those delays by design, LiveKit provides
10:43 a low-latency network infrastructure. In STT-LLM-TTS pipelines,
10:48 the LLM component is often the main source of latency.
10:51 To reduce it, you can use smaller or quantized models
10:55 when self-hosting, or opt for faster inference providers
11:00 if you're relying on LLM APIs. You can also prompt the LLM
11:04 to generate shorter or staged replies to reduce perceived latency
11:08 response time. Now let's see this in action.
11:11 I'm going to show you
11:12 a brief demonstration of the voice agent we use in our Andrew avatar.
11:16 The voice agent backend is running locally on my laptop.
11:19 For the front end, we'll use the LiveKit playground, which is a versatile web
11:23 frontend that makes it easy to test your multimodal AI agent
11:27 without having to worry about the UI until you're happy with your backend.
11:32 Here on the left-hand side, you can see the voice agent code,
11:36 which is very similar to the minimal example code I showed you earlier.
11:40 You'll learn more about voice pipeline agents later in the course. In the back
11:43 end on the right hand side,
11:44 you see the code running locally on my computer in the command line.
11:48 Here's the LiveKit playground running on my browser.
11:51 I've created a project for the course, LiveKit course.
11:57 And now I'm going to connect.
12:00 Hi, how are you?
12:00 Do you want to learn more about machine learning?
12:03 Tell me about AI voice agents.
12:07 AI voice agents are systems
12:09 that use natural language processing and machine learning to understand
12:12 and respond to human speech, providing information or performing tasks.
12:17 Are you interested in building one or learning how they work in more detail?
12:21 Wow, I think it turned out really well.
12:23 It sounds a lot like Andrew.
12:25 Don't you think?
12:26 Okay, let's try
12:27 voice activity detection.
12:29 If this works, we should be able to interrupt
12:31 and the avatar should stop speaking.
12:34 Tell me about speech-to-text models.
12:39 Speech-to-text models
12:40 convert spoken language into written text using techniques like deep learning,
12:44 often leveraging architectures like recurrent neural networks
12:47 or transforming... I meant text-to-speech.
12:51 Text to speech...
12:52 TTS models transform written text into spoken words
12:55 using machine learning algorithm to produce natural-sounding speech.
12:59 Are you curious about how these models can be applied in real-world applications?
13:04 Okay, the voice activity detection allowed us to interrupt.
13:07 This is great.
13:09 Let's go on to the next lesson where you'll learn more about
13:11 the details of the end to end architecture of a voice agent.
```
# 3
```
ranscript
0:02 Like most computer programs you write or applications you build,
0:06 there's a balance to strike between the quality of the output
0:09 and how quickly that output arrives.
0:11 For a voice agent, specifically, the trade-off is between two things.
0:15 One, how quickly can the agent understand your thoughts and respond back to you?
0:20 And two, how well the agent understood your thoughts
0:24 and how helpful was its response.
0:26 In the next lesson, we'll talk in detail about optimizing everything
0:30 behind the agent.
0:31 Right now, though, let's zoom in on the relationship
0:34 between the user's device and the agent running on a server. At the highest level
0:39 what we need to do here is transfer speech,
0:42 in other words, audio data between two computer programs.
0:46 One computer program is running on a user's device,
0:49 it's the application, and the other one is the voice agent running, on a server.
0:54 What's the ideal way for us to do this?
0:56 Let's spend a bit of time learning about networking protocols.
1:00 The internet is built on IP, the internet protocol.
1:04 It's responsible for giving every device an address, just like a house.
1:09 If one device wants to send information to another device,
1:13 kind of like sending physical mail between two houses,
1:16 there are a couple of protocols built on top of IP for doing this.
1:20 TCP and UDP and they each offer different methods of doing so.
1:26 TCP prioritizes reliability over latency.
1:30 We can see our user is sending some speech packets using TCP to the agent.
1:36 The internet is kind of like the world's road system.
1:39 And every packet that is sent out might take a different route
1:43 through the network.
1:44 In this example, packets one and three arrive at the agent first
1:49 while packet two is still in transit.
1:52 TCP will not provide your application in this case,
1:55 the agent, access to packets one and three until packet two arrives.
2:00 If packet two is lost in transmission, which can happen, the TCP protocol
2:05 on the receiver side will ask the sender to resend packet two.
2:10 Meanwhile, your agent program must continue
2:12 to wait until packet two successfully arrives.
2:16 Now, in your mind, imagine a real world voice agent example
2:20 where tens of thousands of audio packets are being sent every second.
2:25 If even a small subset of those packets are lost or slow
2:30 to reach the agent, it can cause the queue to get backed up or stalled.
2:34 You may see this problem in other places referred to as head of line blocking.
2:39 TCP design is problematic for voice
2:42 agent applications, as it ultimately leads to stuttering
2:46 or freezing in audio playback, which is a poor user experience.
2:51 UDP, on the other hand, prioritizes latency over reliability.
2:57 Unlike TCP with UDP, the protocol will hand your agent
3:02 any and all packets the second they're received.
3:05 Concretely, as you can see in the diagram,
3:08 your agent has access to packets one and three immediately.
3:12 Even though packet two is still in transit, that means your agent
3:17 can actually make a choice about what to do in this situation.
3:21 It can wait for packet two, ignore packet
3:25 two even interpolate between packets one and three,
3:28 or maybe just skip packets one and two and start straight from packet three.
3:33 For real-time streaming applications like AI voice agents,
3:37 UDP is a better fit where we have full control
3:40 over what to do in poor networking situations.
3:44 Now that we know UDP is the ideal choice for our use case, how can we use it?
3:49 It's a low-level protocol, so we'd have to write a lot of code to use it directly.
3:54 Fortunately,
3:55 there are higher-level web protocols that are a bit easier to work with.
4:00 There are three that are widely supported across every browser, desktop and mobile,
4:05 and that's HTTP, WebSocket, and WebRTC.
4:09 HTTP builds on top of TCP,
4:13 which we now know isn't the ideal choice for what we're building.
4:17 It's a stateless protocol, meaning it wasn't designed for long
4:21 lived connections where you're constantly streaming data back and forth.
4:25 With HTTP, the sender connects to the receiver, sends some data,
4:30 and then disconnects.
4:31 Ignoring the disadvantages of TCP underneath.
4:34 If we still wanted to use HTTP to exchange voice data between our user and agent,
4:40 we would have to figure out how long to buffer the audio on the sender
4:44 side, establish a connection to the receiver, send
4:47 that buffer along, and then disconnect once it's sent.
4:50 Not only is this tricky to do well, but every time we have to connect
4:54 and disconnect, it takes additional time, which adds overall latency and makes
4:59 handling things like the user interrupting the agent mid speech difficult.
5:04 Lastly, HTTP stands
5:07 for Hypertext Transfer Protocol, not hyper audio
5:12 or hyper voice or hyper speech, which means it doesn't
5:16 have higher-level abstractions for sending audio data back and forth.
5:21 For our use case, exchanging audio data is the primary thing we're doing between
5:26 client and server, so it would be nice to have robust tools for doing that.
5:31 Here's a fun fact.
5:32 Over a decade ago, applications like Siri or Alexa
5:36 actually used HTTP for their voice agents when a user had a question.
5:41 Their client application,
5:42 like the Alexa device, would record their speech to a local file.
5:47 Once the user finished speaking, their client made an HTTP request
5:51 to a server endpoint,
5:53 uploading the audio file containing the user's query to Amazon servers.
5:57 The server application processes
5:59 their query, generated an audio file response
6:02 and sent that file back in an HTTP response.
6:05 Once the audio file was downloaded by the client application,
6:09 it played out through the device's speakers.
6:11 Now that you know more about how HTTP works under the hood,
6:15 you can see why those early voice agents didn't feel that fast or conversational.
6:20 What we need is a stateful architecture, one where we can
6:24 establish a persistent connection between the user and agent,
6:29 and constantly stream audio back and forth.
6:32 Either side can process speech on the fly, just like a human does with their ears,
6:38 and stream speech back as it's generated by the agent or spoken by the user.
6:44 With a stateful system, the agent can build up conversational history
6:48 and detect when the user is done speaking or interrupting it in real time.
6:53 Is there a protocol which can help us accomplish this?
6:56 WebSocket does support persistent connections
6:59 and allows us to stream arbitrary data back and forth.
7:02 But like HTTP, it's built on TCP,
7:07 so it fundamentally suffers from the same problems.
7:10 It also doesn't have any special facilities for sending audio
7:13 data back and forth.
7:15 WebRTC is the other widely supported protocol
7:19 that supports long lift connections and bidirectional data streaming.
7:23 It's built on UDP and was specifically designed
7:27 for transferring audio or video data between two computers. At a protocol level,
7:32 WebRTC measures a network in real time and paces the sending of audio packets
7:38 so that they arrive as smoothly and consistently as possible
7:42 on the receiver side. It also automatically compresses audio
7:46 data before it's sent over the wire, which helps with latency.
7:51 The more data one side needs to send, the longer it takes.
7:55 For example, five seconds worth of speech data that sent uncompressed
8:00 over an HTTP request or streamed with a WebSocket
8:03 becomes just 3% of the data size when it's compressed by the WebRTC protocol.
8:09 Lastly, WebRTC automatically timestamps every packet sent over the wire,
8:15 making handling things
8:16 like interruptions and knowing exactly when that interruption occurred
8:20 trivial. WebRTC is used at scale
8:23 by some of the most widely adopted applications in the world.
8:27 Discord, Google Meet, Zoom, and TikTok
8:30 all use it for real-time audio and video streaming.
8:34 WebRTC does come with some challenges though.
8:37 The first one is complexity.
8:39 It's just really hard to work with.
8:42 This is the full call stack,
8:43 just to establish a one-on-one call between a sender and receiver.
8:48 Another challenge with WebRTC is scale.
8:51 The standard WebRTC protocol is a peer-to-peer protocol. That implies
8:56 that if we use standard WebRTC for our voice agent application,
9:01 your user and agent would be directly connected.
9:04 Speech would stream directly from source to destination over the public internet.
9:10 As we learned earlier, the public internet is kind of
9:14 like the worldwide road system with the equivalent of freeways,
9:17 residential streets, one lane roads, potholes, and yeah, even rush hour.
9:22 Everyone else's data packets are traveling along the same roads as yours.
9:26 The longer your packets have to travel, the more of these road hazards
9:30 they'll end up encountering, and the longer
9:32 it will take for those packets to arrive at their destination.
9:35 One way to solve this problem is to deploy your agent all around the world,
9:40 so all of your users, regardless of where they are,
9:43 have a relatively short path to exchange audio between them and the agent.
9:47 This is a pretty serious
9:48 undertaking though, and it comes with a lot of operational complexity.
9:52 Fortunately, there's infrastructure which solves
9:55 both of the challenges that come with using standard WebRTC.
9:59 LiveKit is an open-source project that makes building and scaling
10:03 a voice agent using WebRTC easier than using HTTP or WebSocket.
10:08 On the client side, LiveKit has open-source SDK
10:12 across every platform you can drop into your application.
10:16 It streamlines establishing a persistent connection
10:19 between your user and agent, and streaming speech back and forth.
10:23 On the server side, LiveKit has an open source
10:26 framework that makes building an AI voice agent simple.
10:29 We'll talk more about the agent side in the next lesson.
10:32 Remember that road system we learned about?
10:35 It turns out that there's another way to avoid traffic.
10:39 Instead of taking the public streets, what if we could drive along
10:43 private tunnels through the internet?
10:45 LiveKit Cloud
10:45 is a set of WebRTC servers distributed
10:49 all around the world that form a global network of tunnels.
10:53 When you're a user wants to stream speech data to your agent,
10:56 they can send their data over the public internet to the nearest
11:00 LiveKit cloud server that hop from user's device
11:03 to the nearest station is short, so there's not much added latency.
11:08 Once packets reach the LiveKit cloud server, they travel through this
11:11 tunnel network on an optimized path with little to no congestion.
11:15 The packets will exit LiveKit's network at the server closest to where
11:19 the agent is running, and make a short final trip to the agent itself.
11:24 In practice, this strategy reduces network latency
11:27 between the user and your agent by about 20 to 50%.
11:31 If you're interested in seeing LiveKit in action,
11:34 the OpenAI team used LiveKit to build ChatGPT
11:37 Advanced voice mode. When you talk to their voice agent
11:40 all of that voice data is going back and forth through LiveKit's cloud network.
11:45 Now that you have a good understanding of what's happening under the hood
11:48 between the user's device and your agent, let's talk
11:52 more about the voice agent and what's going on behind the scenes
11:56 once your user speech reaches the server.
```
# 4
```
Transcript
0:02 In the previous lesson, we discussed
0:04 how we'll connect our client device to the voice agent via WebRTC.
0:09 That's great.
0:10 Your user can now
0:11 speak to your agent with less than 30 to 50 milliseconds of latency.
0:15 But what is a voice agent anyway?
0:18 A voice agent is simply a stateful computer program
0:22 that can consume and process voice data streaming into it, like from a user
0:27 speaking into their phone,
0:28 and it can generate a spoken response to send back to that user.
0:32 Most of the application logic of your agent program
0:35 will be specific to your use case, but voice agents built using LiveKit's Agents
0:40 SDK have a persistent WebRTC connection,
0:44 linking it to one or more client devices.
0:47 LiveKit's Agents SDK
0:49 also takes care of things like managing the conversation context
0:52 and spinning up an instance of your agent for every user that wants to speak to it.
0:57 Every agent instance may keep a database connection for performing actions
1:01 like RAG, interact with libraries or services running on the same machine,
1:06 or make HTTP requests or establish WebSocket connections
1:10 to external services for things like speech to text or LLM inference.
1:15 In the lessons that follow,
1:17 you'll use LiveKit's Agents SDK for building your voice agent.
1:21 You'll build your agent in Python, but you can also use Node.js as well.
1:27 How do we give the agent the ability to listen, think, and speak back to the user?
1:32 Let's zoom in on that part.
1:34 This is what's known as a pipeline
1:37 or cascaded or component model
1:40 voice architecture. As voice input data from a user streams into the agent
1:45 it passes through an ordered series of steps
1:48 before a voice response from the agent is sent back to the user.
1:53 First, the agent relays the user's speech to a smaller speech
1:57 to text AI model, often abbreviated as STT. The STT model
2:02 converts the speech to text in real-time and passes it back to the agent.
2:08 Once the user is done speaking and the agent has the full transcription
2:12 of what the user said,
2:13 it relays that full transcription to an LLM as the user's prompt.
2:18 The LLM takes that prompt and runs
2:22 inference against it. As output tokens are generated,
2:25 They stream back out to the agent from the LLM.
2:28 The agent collects and organizes these tokens. For every sentence
2:33 that it collects,
2:34 the agent will relay that sentence to another, smaller
2:37 text-to-speech model, often abbreviated TTS.
2:41 The TTS model will convert those sentences sent by the agent
2:45 back into speech and stream them to the agent.
2:49 The agent takes those and will relay the bytes of voice data
2:53 as it receives them from TTS back to the client device
2:57 over the persistent WebRTC connection we talked about in the previous lesson.
3:02 And so that's the full end-to-end pipeline that your agent is running through,
3:07 where it takes user speech converts it into text, puts that text
3:11 through an LLM, takes the tokens coming out of the LLM, pipes out through TTS,
3:17 and then from TTS streams the audio back to the user.
3:20 I want to turn our attention now, though, to one of the hardest problems
3:25 in building convincingly human voice agents.
3:29 And that's turn detection. In a human conversation,
3:32 this concept of alternating between speaking and listening is known as turn
3:38 taking. A human is quite good at sort of
3:42 automatically knowing when they should speak or listen.
3:44 Turn detection is a heuristic a voice agent uses
3:48 to know when the user is done speaking, and it can respond.
3:52 Contemporary turn detection systems combine two signals.
3:57 The first is signal processing.
4:00 Is the user actually speaking or not?
4:02 And the second is semantic processing.
4:05 What did the user actually say?
4:08 Here's how this is done
4:09 typically. As user audio streams into the agent,
4:13 it's not just sent to STT, but in parallel
4:17 it's also streamed into something called Voice activity detection, or VAD
4:22 for short.
4:23 VAD analyzes the audio signal.
4:26 It's a small binary classifier neural network
4:30 that simply outputs whether human speech was detected in the input sample or not.
4:35 When the bit flips from human speech detected to not detected,
4:39 VAD starts a timer for a developer configurable number of millisecond
4:45 before firing an event marking the end of the user's turn.
4:49 If VAD detects human speech again before the timer has fired,
4:54 everything is reset. As the user speech is converted
4:57 into text by STT and sent back to the agent,
5:00 the agent also forwards the transcriptions from STT
5:04 to another part of the turn detection system.
5:07 A semantic turn detector model, LiveKit
5:11 has an open-source transformer model that we've trained in-house.
5:15 It takes in a user's transcribed speech and the transcriptions from the last 3
5:20 or 4 previous turns in the conversation, and outputs a prediction whether it thinks
5:25 the user is done speaking or not based on what they've said.
5:29 A VAD detected silence and set a timer to fire the end of an event.
5:34 But the semantic model believes the user isn't done speaking yet.
5:38 For example, if they're just pausing between thoughts, the turn detector
5:42 will delay the timer from firing for a configurable amount of time.
5:46 And now that we have an understanding of turn detection, this is what
5:49 our overall voice agent architecture looks like with it incorporated. Only
5:54 when the turn detection system has fired an end-of-turn event can the agent proceed
6:00 to forwarding the user's transcribed speech to the LLM for inference.
6:05 VAD isn't just used for turn detection either.
6:09 It's also used for interruption handling. To mimic
6:12 the dynamics of two humans having a conversation,
6:16 we need to be able to handle the case when a user interrupts
6:19 the voice agent mid-speech.
6:21 This could happen for a variety of reasons, including
6:25 the LLM may be speaking more than necessary.
6:28 Or the user changed their mind about something.
6:31 The user may want to correct themselves even. Under the hood,
6:35 since the user's speech is already being passed into VAD via
6:39 turn detection, we are using the presence of human speech as opposed to the absence
6:44 of human speech and turn detection to signal an interruption.
6:47 When an interruption occurs, every part of the voice
6:50 pipeline downstream is flushed.
6:52 If the LLM was performing inference at that time that stopped. If there was any
6:57 agent speech being generated by TTS, that's also stopped.
7:02 One final update to our overall architecture here is context management.
7:07 When it's the agent's turn to think and speak
7:11 not only is the most recent transcription of what the user said sent to the LLM,
7:16 but the full history of everything that's been said thus
7:20 far by either party during that session is also sent along.
7:25 This includes things like function calls and their results,
7:29 which is part of most production grade voice agents. LiveKit's agents
7:33 SDK also automatically synchronizes
7:36 the LLM context when the user interrupts the agent.
7:39 It uses timestamps to determine the last thing the user heard
7:44 played back from the agent and aligns the conversation on the agent side
7:48 to this point. And now we've put it all together.
7:51 That's it.
7:52 In the next lesson,
7:53 we're going to take all of these concepts in theory we've discussed here
7:58 and we're going to put them into practice by building a voice agent
8:00 that you can speak to.
```
# 5
```

0:12
/
5:48
Transcript
0:02 In this lesson, we're going to walk through the different building
0:04 blocks of a voice agent from the moment
0:06 someone starts speaking all the way to when the agent responds.
0:09 We'll break down each stage in the pipeline, talk about trade-offs,
0:12 and look at where you can make decisions that really shaped the user experience.
0:16 Whether you're optimizing for speed, quality, or control.
0:19 Understanding how each layer works
0:21 will help you build smarter, more effective voice agents.
0:24 There are two main types of voice agents. First,
0:27 there's the pipeline approach, which Russ walked through in detail earlier.
0:31 And then there's speech-to-speech or real-time agents, which not Nedalina
0:34 introduced back in the first lesson.
0:35 Speech to speech
0:36 agents are usually simpler to implement, and they can sound really natural.
0:40 But in exchange, you give up control.
0:42 Since the model takes in speech and output speech,
0:45 it's harder to see or tweak what's happening in the middle.
0:48 Pipeline agents, on
0:49 the other hand, have more moving parts and yes, more complexity.
0:53 But you get fine grained control over each stage.
0:57 You can see and manage exactly what's coming in and going out
1:00 at each step, which can be a big deal for real-world applications.
1:03 Every system has trade-offs.
1:05 At some point, you'll probably have to choose between latency, quality and cost.
1:09 One of the big advantages of using a pipeline approach is that
1:12 you don't have to make that trade-off globally.
1:14 You can change out different parts of your system based on what matters
1:18 most for your use case and the example that Nedalina gave earlier,
1:21 if you're doing something like restaurant bookings, you might want to optimize
1:25 for LLM reasoning getting the best possible response from the model.
1:29 But if you're handling something like medical triage,
1:31 where accurate transcription is critical, then it might make more sense
1:35 to spend some more of your latency budget on the speech-to-text layer instead.
1:39 All right, let's talk about what really matters when the rubber meets the road.
1:43 Some of the key things to keep in mind as you're designing each of these sections.
1:47 Starting with voice activity detection.
1:49 This component only sends audio when it detects
1:52 someone is actually speaking. That's a big deal.
1:55 It reduces hallucinations from your speech-to-text model,
1:57 and it cuts down on costs
1:59 since you're not sending
2:00 silence for transcription when there's no voice to transcribe.
2:03 Then we've got the speech-to-text layer.
2:05 This is where we need to make some key decisions, like which languages
2:09 we want to support, whether we're doing direct speech translation,
2:12 and whether we want to use a specially trained model
2:14 for recognizing voice in narrower use cases like telephony.
2:18 The final two steps in your voice agent are the LLM layer and text-to-speech.
2:22 Now, the LLM layer is the one that people are already most familiar with.
2:26 This is where we run text
2:28 to text inference to get a response from the large language model.
2:31 If you want to add things like content
2:33 filtering, this is the layer where you do it.
2:35 It's where you'll see
2:36 the biggest latency hits depending on which model you're using.
2:39 And finally, you have text to speech or TTS.
2:42 This layer takes the text from the LLM and turns it back into speech.
2:46 The TTS layer is where you'll choose things like which voice or accent to use,
2:50 and whether you want to apply any pronunciation overrides
2:53 for specific words or phrases that you want spoken in a certain way.
2:57 Now let's put everything we learned into action and get coding.
3:00 First, we'll import the necessary LiveKit
3:02 agent classes and plugins, including OpenAI for speech-to-text.
3:06 And as the inference layer. ElevenLabs for TTS and Cerebras for voice activity detection.
3:11 We'll also import data,
3:12 and so we can get our environment variables
3:14 into memory and logging so that we can see what's going on with our agent.
3:19 We define our
3:20 assistant class that inherits from LiveKit agent class.
3:23 This assistant is given basic instructions about its role,
3:26 and we'll keep track of what's being said so far in the conversation.
3:30 By default, requests that are sent to an LLM for a response or stateless.
3:34 But we want to maintain a conversation with history.
3:36 The assistant
3:37 will keep all of the messages that
3:38 we've sent back and forth, and context, so that it can hold a conversation
3:42 that's aware of the things we've already said.
3:44 It also keeps track of whose turn it is to talk,
3:47 if it can be interrupted, and which, if any, tools that
3:50 it has at its disposal to answer questions for the user.
3:53 Today we're just going to set the instructions parameter
3:56 and inherit all of the other defaults from the base agent.
3:59 Next, we'll define an asynchronous entry point function
4:02 which will run when LiveKit
4:03 tells her agent that it's needed.
4:05 By default, every new room will request an agent.
4:08 Rooms are the primitive that connect agent sessions to users.
4:11 When an agent and a user are having a conversation that's happening inside
4:15 a room.
4:16 Step by step, this entry point function connects to a LiveKit room.
4:20 We define a agent session with all necessary plugins
4:23 that will need to listen and speak to the user,
4:25 and we assign our assistant to the session.
4:29 Finally, we use the LiveKit agent Jupyter
4:31 CLI command to register the application with LiveKit.
4:34 This will allow our agent to be dispatched to rooms.
4:37 When we run, this next cell will be able to have a conversation with our agent.
4:44 Hello there.
4:44 It's wonderful to have you here.
4:46 How can I assist you today?
4:48 Hi there.
4:49 What kind of things can you help with?
4:51 Hi. I'm here to help with a wide range of topics.
4:53 Whether you need information, help solving a problem,
4:56 or just want a chat, feel free to ask about anything.
5:00 Great. So our agent works.
5:01 Now that our agent is running, let's change the voice out for something
5:05 else.
5:08 So we're going
5:08 to scroll back up here to where we define our custom agent.
5:12 And we're going to add a voice ID.
5:20 Now, when we run our agent.
5:23 Hello there.
5:24 It's wonderful to have you here.
5:26 How can I assist you today?
5:27 Hi, Roger, thanks for joining us.
5:29 I love your voice.
5:31 Thank you so much.
5:32 It's great to be here with you.
5:34 How can I make your day better?
5:36 So to recap from this lesson, we were able to get our agent running
5:39 and we were able to change some of the settings
5:41 so that we could have a different voice.
5:43 In the next lesson,
5:43 we'll learn a little bit about metrics and how we can optimize our agents.
```

