# Lesson 4: Voice Agent Components

<div style="background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px">
<p> ðŸ’» &nbsp; <b>Access <code>requirements.txt</code>:</b> 1) click on the <em>"File"</em> option on the top menu of the notebook and then 2) click on <em>"Open"</em>.

<p> â¬‡ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>"File"</em> option on the top menu of the notebook and then 2) click on <em>"Download as"</em> and select <em>"Notebook (.ipynb)"</em>.</p>

<p> ðŸ“’ &nbsp; For more help, please see the <em>"Appendix â€“ Tips, Help, and Download"</em> Lesson.</p>

</div>

<p style="background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px"> ðŸš¨
&nbsp; <b>Different Run Results:</b> The output generated by AI models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>

## Step 1: Import LiveKit Agent Modules and Plugins


```python
import logging

from dotenv import load_dotenv
_ = load_dotenv(override=True)

logger = logging.getLogger("dlai-agent")
logger.setLevel(logging.INFO)

from livekit import agents
from livekit.agents import Agent, AgentSession, JobContext, WorkerOptions, jupyter
from livekit.plugins import (
    openai,
    elevenlabs,
    silero,
)
```

## Step 2: Define Your Custom Agent


```python
class Assistant(Agent):
    def __init__(self) -> None:
        llm = openai.LLM(model="gpt-4o")
        stt = openai.STT()
        tts = elevenlabs.TTS()
        #tts = elevenlabs.TTS(voice_id="CwhRBWXzGAHq8TQ4Fs17")  # example with defined voice
        silero_vad = silero.VAD.load()

        super().__init__(
            instructions="""
                You are a helpful assistant communicating 
                via voice
            """,
            stt=stt,
            llm=llm,
            tts=tts,
            vad=silero_vad,
        )
```

## Step 3: Create the Entrypoint


```python
async def entrypoint(ctx: JobContext):
    await ctx.connect()

    session = AgentSession()

    await session.start(
        room=ctx.room,
        agent=Assistant()
    )
```

## Step 4: Setting up the app to run
- To speak to the agent, unmute the microphone symbol on the left. You can ignore the 'Start Audio' button.
- The agent will try to detect the language you are speaking. To help it, start by speaking a long phrase like "hello, how are you today" in the language of your choice.


```python
jupyter.run_app(
    WorkerOptions(entrypoint_fnc=entrypoint), 
    jupyter_url="https://jupyter-api-livekit.vercel.app/api/join-token"
)
```

## Step 5: Try new voices
Update step 2 with voice id's. For example:  
`tts = elevenlabs.TTS(voice_id="CwhRBWXzGAHq8TQ4Fs17") `


```python
# Roger: CwhRBWXzGAHq8TQ4Fs17
# Sarah: EXAVITQu4vr4xnSDxMaL
# Laura: FGY2WhTYpPnrIDTdsKH5
# George: JBFqnCBsd6RMkjVDRZzb
```

## Experiment with ElevenLabs:
For more information about using Elevenlabs in your voice projects, look for more information at their [website](https://elevenlabs.io/conversational-ai). 



```python

```
