# 6
```

0:02 Voice agents live or die by latency.
0:04 In this lesson, we'll walk through each stage of the voice pipeline
0:08 and look at where delays happen and how to reduce them.
0:10 From speech detection to text generation and back to voice.
0:14 We'll cover the key levers
0:15 that you can pull to make your agent feel fast and responsive.
0:18 A critical part of keeping things fast comes from the client.
0:22 Let's talk a bit more about WebRTC.
0:24 WebRTC is an open-source project that enables
0:27 real-time communication directly in web browsers and mobile apps.
0:30 The key thing here is it lets you share audio, video
0:33 and data without needing any plugins or extra software.
0:36 It uses the Get User Media API to access your device's camera and microphone,
0:41 and also supports screen sharing through the Get Display Media methods.
0:45 And if you want to go beyond just audio and video, you can use
0:48 RTC data channels for direct data exchange.
0:51 All right.
0:52 Let's talk about optimizing the speech pipeline.
0:54 You probably remember the main components here.
0:56 We've got voice activity detection which identifies when someone is speaking.
1:00 Turn detection, which helps manage speaker transitions, and speech
1:04 to text which converts audio into text.
1:06 Now by default VAD in turn detection or blocking.
1:09 And that's intentional.
1:10 We don't want to send audio frames unless we're confident it's actually voice.
1:14 With VAD, we usually lose about 15 to 20 milliseconds
1:17 at the start of each utterance just to confirm that speech is happening.
1:20 Turn detection is a bit different.
1:22 It's not blocking for transcription.
1:24 It listens for the end of a user's turn and fires an event
1:27 when that turn is done, but it doesn't stop transcription
1:30 from happening while someone is still speaking.
1:32 So here's how it works in practice.
1:34 If someone is speaking a paragraph, say five sentences.
1:37 Speech-to-text transcribes continuously in segments
1:40 as the person talks.
1:41 Those first few segments go off to the transcriber in real time.
1:45 Then once turn detection signals that the speaker is done,
1:48 that's when we send the full transcript to the LLM.
1:51 So it's not all waiting on one big blob.
1:53 It's a stream.
1:54 And turn detection just helps us know when to move forward.
1:57 Next up is the LLM stage.
1:59 LLM generate responses token by token, and even the model itself doesn't know
2:03 exactly how long the full response will be until it's finished.
2:06 So it wouldn't make sense for us to wait until the whole response is ready.
2:09 Instead, we stream the output from the LLM to the text-to-speech
2:13 engine as it's being generated.
2:15 The most important metric to track here is time to first token.
2:18 That's how long it takes
2:19 for the model to produce the very first part of its response.
2:22 And it usually defines how long users are waiting before anything starts happening.
2:26 Everything after that happens asynchronously as the LLM keeps generating
2:30 tokens
2:31 we're already passing that text off to the TTS engine to start synthesizing speech.
2:35 So if you want to cut down your overall response time, time to first
2:38 token is where you want to focus your latency optimizations.
2:41 Finally, we have text-to-speech streaming.
2:44 In this stage, we're streaming directly from the LLM to the TTS engine,
2:47 which gives us the best latency.
2:49 Now, unlike the LLM stage where time to first token is the key
2:52 metric, here, the most important thing to watch is time to first byte.
2:55 That's when we actually start hearing audio coming out of the TTS engine.
2:59 The total time it takes to render the full response is less critical,
3:03 as long as rendering happens faster than the engine can speak it.
3:06 Since it takes time to physically speak the utterance, the model has a buffer.
3:10 So again, for TTS performance, the thing to optimize
3:13 is that time to first byte.
3:15 That's what determines how quickly the voice actually starts responding.
3:18 Let's see how fast our agent really is and track some metrics.
3:22 So the first step here is going to be the same as in the last module.
3:25 We're going to import our agent plugins and modules.
3:29 We are going to have to add a few extras
3:31 though. We're going to import the metric classes that
3:33 define the structure of the data collected from each part of the voice pipeline.
3:37 These classes help us access performance information like response
3:40 time, token counts, audio durations, things like that.
3:43 We're also going to import async IO so we can run asynchronous tasks.
3:48 This lets us handle metrics collection and other background work without blocking
3:51 the main flow of the agent.
3:53 Next, we're going to define our agent exactly as we did in the last one.
3:56 But this time we're going to call it the metrics agent.
3:59 Next, we're going to add our metrics collection hooks.
4:02 These handlers let us collect performance data from each part
4:05 of the pipeline and trigger callback functions when metrics are available.
4:08 The first thing that we're adding is our LLM metrics.
4:11 So that's for time to first token and tokens per second.
4:14 Next, we're going to add our STT metrics wrapper.
4:18 And this is going to tell
4:19 us the duration of input and whether streaming was used.
4:22 Next, we'll add our end of utterance metrics wrapper.
4:25 And this one here is going to tell us how long it took us to detect that someone
4:29 was speaking with VAD, and also how long transcription took.
4:33 Lastly, our text-to-speech metrics wrapper.
4:36 This one is going to be time to first byte, total render time.
4:39 This is the one that says how fast the agent is starting to speak back to us.
4:43 Now that we have our wrappers defined, we are going to actually define
4:47 the callback functions
4:48 that are going to collect the metrics and print them to the console.
4:54 First, our LLM metrics.
4:57 The things that we're tracking here are the prompt tokens,
5:00 completion tokens, tokens per second and that time to first token.
5:04 That's so critical.
5:05 Next, we'll add our speech-to-text metrics.
5:09 So the total duration of audio
5:11 and whether or not the response was streamed.
5:14 Next, is our end-of-utterance metrics.
5:16 This is the one that tells us
5:17 how long the speech-to-text took to run and how long that took.
5:21 Lastly is our text-to-speech metrics.
5:23 This is the time to first byte.
5:25 So how long it takes
5:26 until we can actually hear our agent speaking, as well as the duration,
5:29 audio duration, and whether or not the response was streamed.
5:32 Next, we're going to define our entry point.
5:34 Just like we did in the last agent.
5:41 Then we can run our agent.
5:47 Hi. I wanted to see how fast you were.
5:52 Hello.
5:53 I'm designed to respond quickly to your questions and requests.
5:57 How can I assist you today?
5:59 Now, as we scroll down through our logging,
6:01 we can see all of our speech to text metrics.
6:03 We can see our LLM metrics including tokens per second and time to first token.
6:08 And then we can see how long it took for
6:09 those critical first bytes to come through.
6:12 Okay, so let's make a change.
6:14 Right now our time to first token through our LLM was 0.84 seconds.
6:19 But I think we can make that a little bit better.
6:21 We're going to change our LLM model from GPT-4o to
6:25 GPT-4o-mini.
6:26 This is a slightly less capable model than 4o, but it responds a lot more quickly.
6:33 Let's try talking to our agent again
6:35 using 4o-mini.
6:41 Hi there,
6:41 I just wanted to see how fast you are.
6:46 I'm ready to help.
6:47 What do you need assistance with?
6:50 Now if we scroll down
6:51 to look at our LLM metrics, we can see our time to first
6:55 token was almost twice as fast as when we used 4o.
6:59 So just to recap, in this lesson, we rebuilt our agent,
7:02 added all the ability for us to track metrics,
7:05 and then reduced the response time of our LLM by almost half.
```

